---
output: html_document
---

<!-- title: "10 Things You Need to Know About Heterogeneous Effects" -->
<!-- author: 'Methods Guide Author: Albert Fang' -->

Abstract
==
This guide[^1] discusses heterogeneous treatment effects.[^2]

[^1]: Originating author: Albert Fang, 3 Jun 2016. The guide is a live document and subject to updating by EGAP members at any time. Fang is not responsible for subsequent edits to this guide.

[^2]: This guide draws heavily from Alan S. Gerber and Donald P. Green. 2002. *Field Experiments: Design, Analysis, and Interpretation.* New York: WW Norton, and from Don Green's course notes for Experimental Methods at Columbia University.

1 What Is Treatment Effect Heterogeneity?
==
Any given treatment might affect different experimental subjects in different ways. The study of treatment effect heterogeneity is the study of these differences across subjects: For whom are there big effects? For whom are there small effects? For whom does treatment generate beneficial or adverse effects?

2 Testing for Heterogeneity
==
As a first step, one might be interested in whether the variance of the treatment effect across subjects is statistically distinguishable from zero and seek to test the null hypothesis that $Var(\tau_i) = 0$. However, it is not possible to calculate $Var(\tau_i)$ in an experimental setting because you never get to see the treatment effects for any particular individual. Instead, you only get to see outcomes for each person either in the treatment or in the control condition.[^3]

[^3]: This is also known as the Fundamental Problem of Causal Inference (Holland 1986). For more information, see \url{https://en.wikipedia.org/wiki/Rubin_causal_model}..

To illustrate this, we can rewrite $Var(\tau_i)$ as
$$
\begin{aligned}
Var(\tau_i) &= Var(Y_i(1) - Y_i(0)) \\
&= Var(Y_i(1)) + Var(Y_i(0)) - 2Cov(Y_i(1),Y_i(0))
\end{aligned}
$$
The term $Cov(Y_i(1),Y_i(0))$ on the right hand side cannot be estimated in an experiment because we only observe a subject's treated potential outcome or untreated potential outcome, not both.

As an alternative, randomization inference (Fisher 1935) offers a useful method to compare whether the treatment outcome distribution is the same as the control outcome distribution under the null without additional modeling assumptions, asymptotics, or regularity conditions.[^4]

[^4]: For further reading, see Gerber and Green (2012) and Peng Ding, Avi Feller, and Luke Miratrix. 2016. ``Randomization inference for treatment effect variation.'' *Journal of the Royal Statistical Society, Series B.* 

**1. Test the sharp null hypothesis that $Var(\tau_i)=0$.** Under a constant effects assumption (i.e., $\tau_i = \tau, \forall i$), we can test the null hypothesis that $Var(\tau_i)=0$ by testing the null hypothesis that the variance of the treated potential outcome and the untreated potential outcome are equal, or $Var(Y_i(1)) = Var(Y_i(0))$. This is because
$$
\begin{aligned}
Var(Y_i(1)) &= Var(Y_i(0) + \tau_i) \\
&= Var(Y_i(0)) + Var(\tau_i) + 2 \cdot Cov(Y_i(0),\tau_i)
\end{aligned}
$$
and $Var(Y_i(1)) = Var(Y_i(0))$ under the constant effects assumption because $Var(\tau_i) = 2 \cdot Cov(Y_i(0),\tau_i) = 0$. 

To implement the test, first estimate the observed absolute difference-in-variances using the data from the experiment. Then using randomization inference, calculate the $p$-value of observing an absolute difference-in-variances at least as large as the one observed in the actual experiment. 

```{r, message=FALSE, error=FALSE, warning=FALSE}
# sample data generating process
n = 100
Y0 = rep(0:1, n/2)
Y1 = Y0+ 2*(1:n)/n
t = sample(((1:n)<=(n/2)))
Y = t*Y1 + (1-t)*Y0

# calculate absolute value of the difference in variance
DV = function(D, Y) abs(var(Y[D]) - var(Y[!D]))
DV(t, Y)

# calculate p value
p.value = function(t, Y, sims=2000) mean(sapply(1:sims, function(i) DV(sample(t), Y)>=DV(t, Y))) 
p.value(t,Y)

# additional code to calculate power
power = function(t,Y1,Y0,niter) {
  store <- rep(NA, niter)
  for(i in 1:niter){
    t.sim <- sample(t)
    store[i] <- (p.value(t.sim, t.sim*Y1 + (1-t.sim)*Y0) < .05)
  }
  return(mean(store))
} 
power(t=t,Y1=Y1,Y0=Y0,niter=1000)
```

This approach is limited because power for tests of differences in variance is weaker than power for tests of differences in means; thus you might often fail to reject the null hypothesis of constant effects even if there is real heterogeneity in effects. Another potential limitation with this method is that it is uninformative when heterogeneous treatment effects exist but the variances of $Y_i(0)$ and $Y_i(1)$ are equal or when $\tau_i$ varies with $Y_i(0)$. 

**2. Compare treatment and control marginal cumulative distribution functions under the null.** As an alternative to comparing variances under the sharp null, one can also compare the marginal cumulative distribution functions (CDFs) of the outcome between treatment and control under the null hypothesis that the CDFs are shifted by a known average treatment effect $\tau$.

The key change to the randomization inference procedure detailed above is the usage of a different test statistic. Ding, Feller, and Miratrix (2016) suggest using a Kolmogorov-Smirnov (KS) test statistic, which measures the maximum point-wise distance between two curves, to compare the maximum point-wise distance between the treatment and control CDFs under the null if $\tau$ is known. The test statistic is:
$$ t_{KS}(\tau) = \max_Y | \hat{F}_0(Y) - \hat{F}_1(Y + \tau) | $$
where $\hat{F}_Z(\cdot)$ denotes the empirical CDF function under the null for treatment arm $Z$. 

If $\tau$ is not known, as is often the case in practice, Ding, Feller, and Miratrix (2016) suggest plugging in the estimated difference-in-means $\hat{\tau}$ for $\tau$ and employing a `shifted' KS (SKS) statistic instead:
$$ t_{SKS}(\tau) = \max_Y | \hat{F}_0(Y) - \hat{F}_1(Y + \hat{\tau}) | $$

3 Conditional Average Treatment Effects (CATEs)
==
A more structured, theory-driven inquiry of treatment effect heterogeneity involves pre-specifying and investigating conditional average treatment effects (CATE). A CATE is an average treatment effect specific to a subgroup of subjects, where the subgroup is defined by subjects' attributes (e.g., the ATE among female subjects) or attributes of the context in which the experiment occurs (e.g., the ATE among subjects at a specific site in a multi-site field experiment).

4 Interaction Effects: Treatment-by-Covariate versus Treatment-by-Treatment
==
In addition to CATEs, researchers are also interested in treatment-by-covariate interaction effects, or the difference between two CATEs when the covariate partitioning subjects into subgroups is not experimentally manipulated. For example, one might estimate an ATE for male subjects and an ATE for female subjects but actually care about whether the difference in ATEs between the male and female subgroups is statistically distinguishable from zero. To ensure unbiased estimation of CATEs and of interaction effects, the covariate used to partition subjects into subgroups must be a pre-treatment covariate and must be measured using the same procedure for all subjects across experimental groups. Generally you cannot interpret treatment-by-covariate interactions as the causal effect of a change in the covariate value on the ATE if the covariate is not experimentally manipulated.

In contrast to treatment-by-covariate interactions, treatment-by-treatment interactions are differences in CATEs where the personal or contextual attribute partitioning subjects into subgroups is experimentally manipulated. Because the covariate is randomly assigned, treatment-by-treatment interactions may be interpreted causally. Factorial and partial factorial designs allow researchers to randomly assign subjects to different combinations of ``cross-cutting'' treatment conditions and to estimate treatment-by-treatment interactions as allowed by the design.

5 Estimating CATEs and Interaction Effects
==
Estimating CATEs and interaction effects is straightforward. Nonparametrically, the CATE may be estimated by calculating the ATE among subjects in the specific subgroup of interest. Interaction effects may be estimated by differencing relevant CATEs. 

CATEs and interaction effects may also be estimated in a regression framework. Here is an example for a hypothetical experiment evaluating the effect of a job training program on future earnings. Let $Y$ be the outcome (future earnings), $Z$ be the treatment variable (1=job training program, 0=control), and $X$ be a pre-treatment covariate (1=scholarship receipt, 0=no scholarship). The model:
$$
\begin{aligned}
Y_i &= \alpha + \beta Z_i + \gamma X_i + \varepsilon_i \label{null}
\end{aligned}
$$
allows us to estimate the ATE ($\beta$) only. We can add an additional term interacting $Z$ and $X$, which yields
$$
\begin{aligned}
Y_i &= \alpha + \beta Z_i + \gamma X_i + \delta Z_iX_i + \varepsilon_i \label{alt}
\end{aligned}
$$
where the coefficient $\delta$ is the interaction effect and is interpreted as the difference between the ATE of the job training program among subjects receiving a scholarship and the ATE of the job training program among subjects not receiving a scholarship. This has a causal interpretation (i.e., $\delta$ is a treatment-by-treatment interaction) when scholarship receipt is randomly assigned and a non-causal interpretation (i.e., $\delta$ is a treatment-by-covariate interaction) when scholarship receipt is not randomly assigned.

The model also allows us to back out the value of the CATEs. The ATE of the job training program among subjects who do not receive a scholarship is $\beta$. The ATE of the job training program among subjects who receive a scholarship is $(\beta + \delta)$.

6 Hypothesis Testing and Inference
==
To test whether the estimated interaction effect could have occurred by chance, one can use randomization inference to generate a full hypothetical schedule of potential outcomes assuming the ATE is equal to the estimated ATE and calculating how often one obtains a CATE at least as large as (as large as) the observed CATE for two-tailed (one-tailed) tests.

One could also conduct randomization inference in a regression framework. One method suitable for two-sided tests involves using the $F$-statistic as the test statistic, where the null model is denoted by Equation (\ref{null}) and the alternative model is denoted by Equation (\ref{alt}). 

```{r, message=FALSE, error=FALSE, warning=FALSE}
# Sample code for RI using F-stat as test statistic
# Code adapted from: http://isps.its.yale.edu/isps/public/Gerber_Green_FEDAI_2012/
#                    Chapter-9/GerberGreenBook_Chapter9_PlotandFtest_Figure_9_1.R

# Let:
# Y = observed outcome
# Z = treatment assignment (complete randomization)
# X = covariate
n <- 1000
Z <- sample(((1:n)<=(n/2)))
X <- sample(((1:n)<=(n/2)))
Y <- rnorm(n)
numiter <- 100000 #number of iterations for randomization inference

# estimate ATE
estate <- mean(Y[Z==1]) - mean(Y[Z==0])

# construct hypothetical schedule of potential outcomes
# using constant effects assumption where tau_i == estate
Y1 <- Y0 <- Y
Y0 <- Y0 - estate*Z
Y1 <- Y1 + estate*(1-Z)

# estimate CATEs
estcate0 <- mean(Y1[X==0 & Z==1]) - mean(Y0[X==0 & Z==0])
estcate1 <- mean(Y1[X==1 & Z==1]) - mean(Y0[X==1 & Z==0])

lm1  <- lm(Y~Z*X)
lm2  <- lm(Y~Z+X)

Ftest  <- ((sum(lm2$residuals^2)-sum(lm1$residuals^2))/1)/(sum(lm1$residuals^2)/(numiter-4-1))
# or alternatively
# Ftest <- waldtest(lm1,lm2)$F[2]

Fdist <- rep(NA,numiter)

for (i in 1:numiter) {
	Zri <- sample(Z)
	estcate0ri <- mean(Y1[X==0 & Zri==1]) - mean(Y0[X==0 & Zri==0]) 
	estcate1ri <- mean(Y1[X==1 & Zri==1]) - mean(Y0[X==1 & Zri==0]) 
	Yri <- Y0*(1-Zri) + Y1*Zri
	
	lm1ri  <- lm(Yri~Zri*X)
	lm2ri  <- lm(Yri~Zri+X)

	Fdist[i]  <- ((sum(lm2ri$residuals^2)-sum(lm1ri$residuals^2))/1)/(sum(lm1ri$residuals^2)/(numiter-4-1))
	# or alternatively
	# Fdist[i] <- waldtest(lm1ri, lm2ri)$F[2]	
	}

#p-value
mean(Fdist >= Ftest)
```

For one-sided tests, the coefficient on the interaction term may be used as the test statistic, given the appropriate model.

7 Multiple Comparisons
==
Researchers interested in heterogeneous treatment effects are likely to encounter the problem of multiple comparisons. The multiple comparisons problem is where researchers obtain statistically significant interactions and overreject the null hypothesis simply by chance when conducting numerous subgroup analyses. 

In addition to reducing the number of tests conducted while keeping fixed the target $p$-value (for example, by pre-specifying subgroup analyses or in the case of multiple outcomes, combining multiple outcome measures within a domain into a summary index measure[^5]), another approach to deal with problems with inference arising from multiple testing is to adjust the target $p$-value.

[^5]: See Anderson (2008), who presents this method in greater detail.

**7.1 Familywise error rate (FWER) control methods**

Familywise error rate (FWER) control methods limits the probability of making any type I error given the number of tests conducted. Suppose one had $k$ hypotheses, $H_1, H_2, ..., H_k$, and $j$ of the $k$ hypotheses are true where $j \le k$. The familywise error rate is the probability that at least one of the $j$ true hypotheses is falsely rejected. The FWER increases in the number of hypotheses tested. To account for this, the target $p$-value is adjusted to reduce the probability of a false rejection.

*A. Bonferroni correction:* The most conservative FWER control method is the Bonferroni correction, which divides the target $p$-value by the number of hypothesis tests conducted. Thus if one originally sets a test size $\alpha=0.05$ and examines $q=20$ interaction effects, then the target $p$-value is adjusted from 0.05 to $(0.05/q) = 0.05/20 = 0.0025$. This approach has serious limits because one quickly loses statistical power with just a few tests.

*B. Free step-down resampling:* An alternative FWER control method is the free step-down resampling method (Anderson 2008). This method is advantageous because it computes an exact probability rather than an upper bound, removes hypotheses that are rejected from the family being tested to increase power for remaining tests, and incorporates dependence between outcomes.

To implement this procedure to address multiple testing of heterogeneous effects, do the following steps which are adapted from Anderson (2008): 

1. Given $K$ null hypotheses in a family of hypotheses (one null hypothesis corresponding to each interaction of interest, e.g., $H_{0k}: \beta_k = 0, k \in K$), choose a test statistic $t(Z,Y,X)$ to use across all hypotheses and sort the observed vector of test statistics $t_1, ..., t_k$ in order of decreasing statistical significance (e.g., if the test statistic is the $t$ statistic then sort in decreasing order, if the test statistic is the $p$-value then sort in increasing order).

2. Construct a full hypothetical schedule of potential outcomes under the null hypothesis of no treatment effect for L $\ge$ 100,000 permutations of treatment assignment.

3. Calculate a vector of simulated test statistics $t_1^*, ..., t_K^*$ for each of the simulated treatment assignment vectors.

4. For each vector of simulated test statistics, enforce monotonicity and compute $t_r^{**} = \max \lbrace t_r^*, t_{r+1}^*, ..., t_K \rbrace$, which is the most ``extreme'' test statistic across a given set of tests for a given the permuted treatment assignment vector.

5. For each of $k$ hypotheses, tabulate $S_r$ the number of simulated iterations where $t_r^{**} > t_r$, then compute an initial adjusted $p$-value for each hypothesis, $p_r^{fwer*} = S_r / L$, which is the percent of simulated iterations where the maximum simulated test statistic (across related hypothesis tests) is at least as large as the observed test statistic.

6. Enforce monotonicity a final time: Calculate $p^{fwer} = \min \lbrace p_r^{fwer*}, p_{r+1}^{fwer*}, ..., p_k^{fwer*}\rbrace$.

Here is a worked example using simulated data with a binary treatment variable $Z$, a continuous outcome variable $Y$, and four covariates $X_1, X_2, X_3, X_4$ that represent four indicators measures that tap the same pre-defined, pre-treatment characteristic. Suppose a researcher wishes to explore heterogeneous effects by interacting treatment with each of the four covariates and estimates the model:
$$
\begin{aligned}
Y_i &= \alpha + \beta_1 Z_i + \beta_2 X_{1i} + \beta_3 X_{2i} + \beta_4 X_{3i} + \beta_5 X_{4i} \\
&\enspace \hspace{.5cm} + \beta_6 (Z_i*X_{1i}) + \beta_7 (Z_i*X_{2i}) + \beta_8 (Z_i*X_{3i}) + \beta_9 (Z_i*X_{4i}) + \varepsilon_i
\end{aligned}
$$

But the researcher is concerned about multiple comparisons if they test the null hypotheses $\beta_6=0$, $\beta_7=0$, $\beta_8=0$, and $\beta_9=0$. We define this set of hypotheses as a family of hypotheses and seek to control the FWER. We calculate a FWER adjusted $p$-value by implementing the above steps using the following code.

```{r, message=FALSE, error=FALSE, warning=FALSE}
rm(list=ls(all=TRUE))
library(MASS) # needed to generate correlated covariates

# Make fake experimental data
set.seed(1234567)
n <- 5000 # Number of subjects
# Create 4 correlated covariates (these indicators all tap the same construct)
xs <- mvrnorm(n, mu=c(0,5,5,0), Sigma=matrix(c(5,1,2,4,
                                               1,5,1,3,
                                               2,1,15,2,
                                               4,3,2,5), ncol=4, byrow=TRUE)) 
Y0 <- sample(0:1, n, replace=TRUE) # Untreated potential outcomes
Y1 <- Y0 + 2*rnorm(n, xs[,1], 3) + 5*rnorm(n, xs[,2], 15) # Treated potential outcomes
Z <- sample(((1:n)<=(n/2))) # Treatment assignment
Y <- Y1*Z + Y0*(1-Z) # Apply switching equation
d <- data.frame(Y=Y,Z=as.numeric(Z),X1=xs[,1],X2=xs[,2],X3=xs[,3],X4=xs[,4]) # Assemble data frame

# Variables: Y=outcome, Z=treatment assignment, inc=income decile (10-level), edu=education level (5-level)
# Uncomment the next two lines to check fake data
# head(d)
# plot(d[,paste0("X",1:4)])

# Analyze data from experiment: model includes treatment x covariate interactions
# Assume heterogeneous effects are in the same family of hypotheses
# Grab observed test statistics on interaction terms
fit <- lm(Y ~ Z + X1 + X2 + X3 + X4 + Z:X1 + Z:X2 + Z:X3 + Z:X4, data=d)
summary(fit)
t_obs <- abs(summary(fit)$coefficients[-c(1:6),3]) # Test statistic: absolute value of t-statistic

# Given k = 4 hypotheses, sort test statistics in order of decreasing significance
# (e.g., decreasing absolute t-value)
# Denote r as the original "significance" rank
sig_order <- order(t_obs, decreasing=TRUE)
t_obs[sig_order] # check order

# Create 100,000 permutations of treatment assignment (note: because we assume
# ATE=0 for all subjects under the sharp null, Y1 = Y0 = observed Y under the sharp null)
niter <- 100000
zperms <- sapply(1:niter, function(j){
  d$Z[sample(1:nrow(d), nrow(d))]
})
dim(zperms) # rows are observations, columns are treatment assignment permutations
# zperms[,1]

# Calculate vector of test statistics under each simulated vector of treatment assignments
sim_t <- NULL
for(i in 1:ncol(zperms)){
  sim_fit <- lm(d$Y ~ zperms[,i] + d$X1 + d$X2 + d$X3 + d$X4 +
                  zperms[,i]:d$X1 + zperms[,i]:d$X2 + zperms[,i]:d$X3 + zperms[,i]:d$X4)
  sim_t[[i]] <- abs(summary(sim_fit)$coefficients[-c(1:6),3])
}
sim_t <- do.call(rbind, sim_t)
dim(sim_t)

# Compute: t_r^{**} = max{t_r^*, t_{r+1}^*, ..., t_k^*} where r is the observed "significance" rank
trss <- apply(sim_t, 1, max) # trss = t_r^{**}

# For each hypothesis, tabulate S_r, the number of times that t_r^{**} > t_r
S <- matrix(NA, nrow=nrow(sim_t), ncol=ncol(sim_t))
for(i in 1:nrow(sim_t)){
  S[i,] <- trss[i] > t_obs
}
head(S)

# Compute p-values: p_r^{fwer*} = S_r / L
p_fwer <- apply(S, 2, sum) / niter
p_fwer

# Enforce monotonicity a final time, choose FWER adjusted p-value
# Select p_r^{fwer} = min{ p_r^{fwer*}, p_{r+1}^{fwer*}, ..., p_k^{fwer*} } as the FWER p-value
p_fwer_adj <- min(p_fwer)
p_fwer_adj
```

**7.2 False discovery rate (FDR) control methods**

False discovery rate (FDR) control methods control the expected proportion of rejections that are type I errors. Formally, $FDR = E[Q = V/t]$ where $V$ is the number of false rejections and $t$ is the total number of rejections, $t=V+U$, and $U$ is the number of correct rejections. 

The basic procedure developed by Benjamini and Hochberg (1995) involves the following steps to control the FDR. Like in the setup to control the FWER, specify $k$ hypotheses $H_1, ..., H_k$ and sort the hypotheses in decreasing order of significance such that $p_1 < p_2 < ... < p_k$. Let $q \in \lbrace (0,1)$ be the level at which the FDR is controlled. Let $c$ be the largest $r$ for which $p_r < (q_r/k)$. Beginning with $p_k$, check whether each $p$-value meets the condition $p_r < (q_r/k)$. When one does, reject it and all smaller $p$-values. The procedure controls FDR at level $q(k_0/k)$ where $k_0$ is the number of true null hypotheses.

A two-stage procedure by Benjamini, Krieger, and Yekutieli (2006) allows one to go further and estimate the number of true hypotheses to achieve sharpened FDR control. First, apply the basic procedure by Benjamini and Hochberg (1995) at level $q' = q/(1+q)$ and let $c$ be the number of hypotheses rejected. If $c=0$, stop; else let $\hat{k}_0 = k - c$ and apply the basic procedure at level $q^* = q'k/\hat{k}_0$.

To obtain the smallest level $q$ at which a hypothesis would be rejected, perform either the basic procedure or the two-stage procedure for all possible $q$ levels and record when each hypothesis ceases to be rejected.

8 Use a Pre-Analysis Plan To Reduce the Number of Interactions Tested
==
One can also reduce the number of interactions under consideration for hypothesis testing by pre-specifying the interaction effects of interest in a registered pre-analysis plan. Additional subgroup analyses can be conceptualized and specified as exploratory or descriptive analyses in the pre-analysis plan.

9 Automate the Search for Interactions
==
Machine learning methods are useful to automate the search for systematic variation in treatment effects. These automated approaches are attractive because they minimize researchers' use of ad hoc discretion in selecting and testing interactions, and are useful for conducting exploratory analyses.

Several popular different machine learning methods include support vector machines (R package [FindiIt](http://cran.r-project.org/web/packages/FindIt/index.html)),[^6] Bayesian additive regression trees (R package [BayesTree](http://cran.r-project.org/web/packages/BayesTree/index.html)),[^7] and kernel regularized least squares (R package [KRLS](http://cran.r-project.org/web/packages/KRLS/index.html)).[^8]

[^6]: See, for example, Imai, Kosuke and Marc Ratkovic. 2013. "Estimating Treatment Effect Heterogeneity in Randomized Program Evaluation." *The Annals of Applied Statistics.* 7(1):443-470.

[^7]: Chipman, H.A., E.I. George, and R.E. McCulloch. 2010. "BART: Bayesian Additive Regression Trees" *Annals of Applied Statistics.* 4: 266-298. Green, Donald P and Holger L Kern. 2012. "Modeling heterogeneous treatment effects in survey experiments with Bayesian Additive Regression Trees." *Public Opinion Quarterly.* 76(3): 491-511.

[^8]: Jens Hainmueller and Chad Hazlett. 2013. "Kernel Regularized Least Squares: Reducing Misspecification Bias with a Flexible and Interpretable Machine Learning Approach." *Political Analysis.*

In addition to single machine learning methods, ensemble methods may be used. Ensemble methods estimate a weighted average of multiple machine learning estimates of heterogeneous effects where weights are a function of out of sample prediction performance.[^9]

[^9]: See van der Laan, Mark, Eric Polley, and Alan Hubbard. 2007. "Super Learner." *Statistical Applications in Genetics and Molecular Biology.* 6(1); Grimmer, Justin, Solomon Messing, and Sean J. Westwood. 2014. "Estimating Heterogeneous Treatment Effects and the Effects of Heterogeneous Treatments with Ensemble Methods." Working Paper.

10 A Note on Interactions between Treatment and Post-Treatment Covariates
==
The discussion thus far has assumed treatment effect heterogeneity involve pre-treatment covariates to ensure unbiased estimation of CATEs and treatment-by-covariate interaction effects. 

Some researchers may be interested in post-treatment effect modification, or the interaction between a treatment and a post-treatment covariate. Conditioning on a post-treatment covariate is not advised, because biased estimation of both the main effect and interaction effects is likely when a post-treatment covariate is included as a regressor. This is especially likely when the covariate is affected by the treatment.

There is a burgeoning body of methodological research on the conditions under which CATEs involving post-treatment covariates are identified. These methods rely on model-based identification (but not nonparametric identification) building on generalized structural mean models (GSMM), which were originally developed to analyze randomized trials with noncompliance[^10] but have since been adapted to study treatment effect heterogeneity involving post-treatment covariates.[^11]

[^10]: Robins, J. M. 1994. "Correcting for Non-compliance in Randomized Trials Using Structural Nested Mean Models." *Communications in Statistics-Theory and Methods* 23: 2379-2412.

[^11]: For further reading, see Vansteelandt, S. and Goetghebeur, E. 2003. "Causal Inference with Generalized Structural Mean Models," *Journal of the Royal Statistical Society, Series B* 65, 817-835; Vansteelandt, S. and Goetghebeur, E. 2004. "Using Potential Outcomes as Predictors of Treatment Activity Via Strong Structural Mean Models." *Statistica Sinica.* 14, 907-925; Vansteelandt, S. 2010. "Estimation of controlled direct effects on a dichotomous outcome using logistic structural direct effect models." *Biometrika*, 97, 921-934; Stephens, Alisa, Luke Keele, and Marshall Joffe. 2014. "Generalized Structural Mean Models for Evaluating Depression as a Post-treatment Effect Modifier of a Jobs Training Intervention." [Working Paper](http://lukekeele.com/wp-content/uploads/2016/03/SMM-Int.pdf)