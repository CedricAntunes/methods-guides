---
title: "10 Things to Know About Covariate Adjustment"
author: 'Methods Guide Author: Lindsay Dolan'
output: html_document
---

Abstract
==
This guide[^1] will help you think through when it makes sense to try to “control for other things” when estimating treatment effects. Economists control a lot, medical science use controls less, and political scientists control less clearly. We focus on the big ideas and provide examples and tools you can use in R.

[^1]: Lead author: Lindsay Dolan. Dolan is not responsible to subsequent edits to this document.

1 What it is
==
“Covariates” are other characteristics (besides treatment) of your experimental subjects. When you run an experiment, you are primarily interested in collecting data on outcome variables you expect to change as a result of your intervention, e.g. expenditure decisions, attitudes toward democracy, contribution to a public good in a lab experiment. But it’s also a good idea to collect data on characteristics of subjects before they received the treatment, e.g. gender, level of education, ethnic group. If you do this you can explore how treatment effects are different across different types of subjects (see 10 Things about Heterogeneous Treatment Effects, forthcoming). But doing this also lets you perform covariate adjustment.

You do covariate adjustment when you include this extra data on potentially explanatory variables when estimating treatment effects. Often this is done to improve precision. The outcomes you may be interested in, such as attitudes toward democracy, are likely to be influenced by other factors beside your treatment, even when there is a treatment effect. Accounting for variables like gender will allow you to set aside the variation in your dependent variable (your outcome) that is predicted by these background characteristics, so that you can identify the relationship between your treatment and your dependent variable with greater precision. When your findings are more precise, you can have greater confidence in your estiamtes of treatment effects.

Because improving the precision of your estimate of a treatment effect is so desirable, researchers often collect extensive data on covariates. These are frequently gathered in the form of a baseline survey, which is issued before an experimental intervention takes place. Baseline surveys can ask individuals who are about to be assigned to either treatment or control about a number of factors that, in addition to their treatment status, are expected to influence the outcomes that will later be observed. A similar exercise is a pre-test. When a treatment is expected to influence an individual’s performance on a certain task, measuring their performance before treatment can provide a useful predictive source of data (so long as the occurrence of the pre-test does not itself affect a subject’s performance on a similar test administered after the treatment is or is not administered — see “7. When not to do it”).

2 How to do it at the design stage (blocking)
==
The best way to control for covariates is to use block randomization to do it at the design stage even before you start your experiment. Block randomization enables you to create treatment and control groups that are balanced on certain covariates. For example, you might expect that gender and income predict the outcome variable in your experiment. If you use block randomization, it will ensure that treatment and control groups have equal proportions of female/high-income, female/low-income, male/high-income, and male/low-income populations. Specifying this means that it will be impossible to find a correlation between gender or income and your treatment, so there will be no reason to control for gender or income when you are analyzing your results.

If you want more information on blocking and how to implement this in R, see [here](http://egap.org/methods-guides/10-things-you-need-know-about-randomization#2block).

A word of caution: if your probability of assignment to treatment varies by block, then you need to control for the block in your analysis.

3 How to do it in a regression
==
Sometimes you do not have the opportunity to implement a blocked experimental design (for example, if you join a project after random assignment has already been determined) or you would prefer to simplify your randomization scheme to reduce opportunities for administrative error. You can still adjust for covariates on the back end by using multiple regression. Remember that in a simple regression — when you regress your outcome on just your treatment indicator — the coefficient on your treatment indicator is just a difference-in-means: it’s the average effect your treatment has on the outcome. When we add covariates to the model, the coefficient on your treatment indicator is still your difference-in-means representing the treatment effect, but it is the estimated difference-in-means once we’ve predicted what our outcomes should look like for different individuals given their covariates. In other words, by using our knowledge of subjects’ background characteristics, we are better able to predict what their outcomes should be independent of treatment and therefore better estimate the overall average treatment effect.

To adjust for covariates through multiple regression, use the model:

$$Y_i = \alpha + \beta D_i + \gamma X_i + \epsilon_i$$

where $Y_i$ is your outcome variable, $D_i$ is your treatment variable, and $X_i$ is your covariate. The remainder, $ϵ_i$ is your disturbance term — the leftover unexplained noise.

Even better (see Lin 2013) include interactions with your treatment (and for ease of interpretation normalize the covariate to have zero mean):

$$Y_i = \alpha + \beta D_i + \gamma X_i + \delta X_I*D_i + \epsilon_i$$

Recall that in the blocked design described above, it was important to account for varying probabilities of assignment to treatment by block. Analogously, if subjects received different probabilities of assignment to treatment based on their covariates, then our multiple regression model would need to account for this. Suppose you encounter an experiment in which subjects received different probabilities of assignment to treatment based on their covariates. First ask yourself: did these different probabilities of assignment to treatment result from a design in which the experiments blocked on covariates and had probabilities that varied by block? If so, you should calculate each unit’s individual probability of assignment to treatment based on its block, then calculate the inverse of these probabilities, and use these as the weights in a weighted regression. In R, this is done by specifying weights= as an argument in the command lm().

4 Why to do it
==
Collecting and adjusting for covariates is not necessary for an unbiased estimator, but under some conditions, which will be described below, it improves precision.[^2] In addition it can also help with conditional unbiasedness (see point 6 below).

[^2]:  A brief review of these concepts: Our goal in an experiment is to estimate a treatment effect that is unbiased and precise. We reduce bias in our experiment by focusing on how we go about calculating the treatment effect; for example, we make sure that treatment was randomly assigned, and if it hadn’t been, we’d know that our difference-in-means estimator will be biased because it’s reflecting the factors that caused individuals to select into or to be selected into treatment. In other words, if we repeated the experiment a thousand times, we would find that our treatment effect on average is higher or lower than the actual underlying treatment effect. An unbiased estimator (which we get when we correct our experiment and reestablish random assignment!) can produce individual estimates of the treatment effect that are too high or too low, but when we take the average of a thousand treatment effects across a thousand experiments, we will recover the true one. But we also like our estimates to be precise. Precision refers to how variable our estimates of the treatment effect are across those thousand experimental replications. We would say that our estimator is precise if we get lots of estimates of the treatment effect that are close to the true treatment effect, and imprecise if we see a wide spread. Imagine that you are throwing a dart at a dart board. If you hit the center of the dart board on average but your shots are often frequently far from the mark, you have an unbiased but imprecise estimator. If you hit the center of the dart board every single time, your estimator is both unbiased and extremely precise.

When treatment is randomly assigned, the difference-in-means estimator is an unbiased estimator of the treatment effect, because the only thing that should systematically differentiate your treatment and control groups is the assignment of treatment. For this reason, it is never necessary to conduct a baseline survey or to have data on covariates in order to estimate an unbiased treatment effect.

However, different random assignments can produce different compositions of treatment and control groups along background covariates. In some random assignments, subjects with similar covariates that are predictive of their outcomes may randomly clump together in treatment or control groups. You might get several young unemployed men in one group and several older conservative women in another group. For this reason, the estimate of the treatment effect will be subject to “sampling variability,” meaning you’ll get estimates of the treatment effect that were generated from an unbiased estimator, but happened to miss the mark quite severely.[^3] A high sampling variability contributes to noise (imprecision), not bias.

[^3]: “Sampling variability” refers to the spread of estimates that will be produced just because of the different random assignments that could have been drawn–will this spread be large or will it produce balanced treatment and control groups consistently? When the luck of the draw on random assignment produces a treatment group with several As in it and a control group with several Bs in it, it is more difficult to separate background characteristics (A and B) from treatment assignment as the predictor of the observed outcomes.

Controlling for these covariates, which are known to be predictive of potential outcomes, corrects for this. Take a look at the following example, which is loosely based on the Gine and Mansuri experiment on female voting behavior in Pakistan. In this experiment, the authors randomized an information campaign to women in Pakistan to study its effects on their turnout behavior, the independence of their candidate choice, and their political knowledge. They carried out a baseline survey which provided them with several covariates.

The following code imitates this experiment by creating fake data for four of the covariates they collect: whether the woman owns an identification card, whether the woman has formal schooling, the woman’s age, and whether the woman has access to TV. It also creates potential outcomes (the outcome an individual would show if she were treated versus the outcome she would show if untreated) for a continuous measure of the extent to which a woman’s choice of candidate was independent of the opinions of the men in her family. The potential outcomes are correlated with all four covariates, and the built-in “true” treatment effect on the independence measure here is 1. In other words, when we regress the observed outcome on treatment assignment, we should see a coefficient of 1 on “Z.” To figure out whether our estimator is biased or not, we simulate 10,000 possible treatment assignments, and then regress the 10,000 simulations of outcomes we would observe on those treatment assignments, with and without controlling for covariates. If the average of the 10,000 coefficients on our treatment variable is 1, then our coefficient must be unbiased, because although each individual randomization may be higher or lower than one, on average, we observe a treatment effect of 1.

```{r, error=FALSE, message=FALSE, warning=FALSE, eval=FALSE}
rm(list=ls())

set.seed(20140714)
N <- 2000
id <- seq(from=1,to=N)

# define pre-treatment covariates
ownsidcard <- rbinom(n=N,size=1,prob=.18)
hasformalschooling <- rbinom(n=N,size=1,prob=.6)
age <- round(rnorm(n=N, mean=37, sd=16))
age[age<18] <- 18
age[age>65] <- 65
TVaccess <- rbinom(n=N,size=1,prob=.7)

#define potential outcomes correlated with pre-treatment covariates
indepchoiceY0 <- round(ownsidcard+2*hasformalschooling+3*TVaccess+log(age))
indepchoiceY1 <- indepchoiceY0+1

#assign treatment 10000 times
Z_mat <- replicate(10000,ifelse(1:N %in% sample(1:N,1000),1,0))

#observe 10000 simulations of potential outcomes
indepchoice_mat <- indepchoiceY1*Z_mat+indepchoiceY0*(1-Z_mat)

#estimate ATE on indepchoice
ate <- function(Y,Z) summary(lm(Y~Z))$coef[2,1]
ate.covs <- function(Y,Z)
  summary(lm(Y~Z+ownsidcard+hasformalschooling+age+TVaccess))$coef[2,1]
dist <- rep(NA, 10000)
dist.covs <- rep(NA, 10000)
for (i in 1:10000) {
  dist[i] <- ate(indepchoice_mat[,i],Z_mat[,i])
  dist.covs[i] <- ate.covs(indepchoice_mat[,i],Z_mat[,i])}

#look at bias and precision in our results
mean(dist) # unbiased!
mean(dist.covs) # unbiased!
sd(dist) # less precise
sd(dist.covs) #more precise
```

Indeed, both models–with and without covariates–yield the true treatment effect of 1. When we ran this regression without covariates, our estimated average treatment effect was 1.00085, and with covariates, it was 1.00013.

The real gains come in the precision of our estimates. The standard error, or the standard deviation of the sampling distribution, of our treatment effect when we ignore covariates was .076. When we include covariates in the model, though, our estimate becomes much tighter: the standard error of the treatment effect is .013. Because our covariates were prognostic of our outcome, including them in the regression explained some noise in our data so that we could tighten our estimate of the treatment effect.

5 How it can help with precision
==
When is adjusting for covariates most likely to improve precision?

Covariate adjustment will be most helpful when your covariates are strongly predictive (or “prognostic”) of your outcomes. Covariate adjustment essentially enables you make use of your knowledge of strong relationships between background characteristics and your outcome so that you can better identify the relationship between your treatment and your outcome. But if those relationships between background characteristics and your outcome are fairly weak, covariate adjustment won’t do you much good. The covariates you will want to adjust for are the ones that are strongly correlated with outcomes.

The following graph demonstrates the relationship between how prognostic your covariate is and the gains you get from adjusting for it. On the y-axis is the mean squared error (MSE), which measures the average of the squares of the difference between the estimator and what is estimated. We want our MSE to be small, and covariate adjustment should help us do this. 

```{r, message=FALSE, warning=FALSE, error=FALSE}
mse = function(n, rho=0, sims = 10000, control = TRUE) {
  t = 1*(sample((1:n)<=(n/2)))
  e =sapply(1:sims, function(j) {
    Y0 = rnorm(n)
    X = rho*Y0+sqrt(1-rho^2)*rnorm(n)
    if(!control) X=rep(1,n)
    Y = Y0 + t
    return( (1-coef(lm(Y~t+X))["t"])^2)
  })
  return(mean(e)^.5)
}
 
n = (6 + 2^(1:10))
 
E  = sapply(n, mse, control =FALSE)
E0 = sapply(n, mse)
E1 = sapply(n, mse, rho=(.5))
E2 = sapply(n, mse, rho=(.9))
 
plot(n, E, type = "l", ylab = "RMSE", xlim = c(min(n),max(n)), ylim = c(0,.5))
  lines(n, E0, col = "yellow")
  lines(n, E1, col = "orange")
  lines(n, E2, col ="red")
  legend(700, y=.4, c("No controls", expression(paste(rho, "=0")), expression(paste(rho, "=0.5")), expression(paste(rho, "0=.9"))),
  col=c("black", "yellow","orange", "red"), lty = 1, lwd=2)
 
png("n.png", width = 600, height = 480, res=100, units="px")
```

The black line shows the MSE when we don’t adjust for a covariate. The red line shows the MSE when we adjust for a highly prognostic covariate, one that strongly predicts our outcomes. You can see that the red line is always below the black line, meaning that the MSE is always lower when you adjust for a prognostic covariate. The orange line represents the MSE when we adjust for our covariate, but that covariate is less predictive of our outcomes. We still are getting gains in precision relative to the black line, but not as much as we did with the red line. Finally, the yellow line shows what happens if you control for your covariate, but it’s not at all predictive of your outcomes. The yellow line is almost identical to the black line: you received no improvement in precision by controlling for a non-prognostic covariate. This exercise demonstrates that you’ll get the most gains in precision by controlling for covariates that strongly predict outcomes. Identifying whether covariates are “prognostic” of outcomes is sometimes more art than science, precisely because we cannot observe both the treated and the untreated potential outcome for each subject! You may want to regress your outcome on your covariate in just the treatment group or just the control group and see whether a clear story emerges: does this covariate seem to significantly affect your outcome in either group? If there is a strong literature on your topic of interest, you can also use previous works that have identified strong correlations in a variety of contexts to back your claim that a covariate is prognostic, but you will need to make the case for why this is true of your particular population as well.

6 How it can help with bias
==
Though surprising, if a randomly assigned treatment is correlated with your covariates, this does not introduce (unconditional) bias. This is because we know that random assignment, in expectation, will produce treatment and control groups that are “balanced” along various characteristics; there is no systematic difference between the two groups except for the receipt of treatment.

But in practice, we often get a draw that produces imbalance on one or more characteristics between the two groups. Perhaps one group is slightly more educated, or one group votes at a slightly higher rate, or one group is slightly more altruistic. It is likely that in these conditions we will observe a treatment effect that is “wrong” in one direction or the other — too low or too high, depending on how the imbalance tilts. Remember that because of random assignment, we would not, ex ante, call our estimator biased because in expectation, over several replications of the same experiment, we will retrieve on average the right treatment effect. But often, we are observing only one iteration of our experiment, so is that single estimation of the treatment effect considered to be biased?

Ex post, we would call that estimation of the treatment effect “conditionally biased.” In other words, conditional on the type of draw we got, our expected estimate of the treatment effect does not correspond to the true average treatment effect. For example, we might worry about conditional bias if we only observed experiments in which the treatment group was significantly more educated from the control group.

The good news is that if the bias results from an imbalanced draw and not because of some feature that un-randomized our assignment, we will be able to retrieve the unbiased estimate of the treatment effect so long as we control for the covariate on which we observe imbalance.[^4] The figure below illustrates conditional and unconditional bias and how including covariates can help address conditional bias. In the example there is a background covariate that is highly correlated with potential outcomes. The true treatment effect is 1 but we see that unless a control is introduced the estimated treatment effect depends on the (accidental) correlation between the covariate and the treatment.

[^4]: Gerber, Alan S. and Donald P. Green. Field Experiments: Design, Analysis and Interpretation. New York: Norton,  2012, p. 109

```{r, message=FALSE, error=FALSE, warning=FALSE}
# Define function to apply over treatment assignments ---------------------
ATE.est <- function(Z, X, Y0, Y1){
  Y = Z*Y1 + (1-Z)*Y0
  return(c(
    covariate.corr = cor(X,Z),
    uncond.ate = as.numeric(lm(Y ~ Z)$coefficients[2]),     
    cond.ate = as.numeric(lm(Y ~ Z + X)$coefficients[2]),  
    true.ate = mean(Y1-Y0)
  ))
}
 
# Start function ----------------------------------------------------------
cond.bias <- function(N=40,sims=8000,seed=1000){
  set.seed(seed)  
  if(N%%2!=0)stop(print("Please enter an even number for N."))
  
  # Generate ate on new data, same DGP  -----------------------------------------------------------
  ate = function(i){
    X  = runif(N)
    Y0 = X+.14*rnorm(N)
    Y1 = X+1+.14*rnorm(N)
    Z  = sample(rep(0:1,N/2))
    return(ATE.est(Z, X, Y0, Y1))
  }
 
  
  # Do many times ----------------------------------------------
    out <- data.frame(t(sapply(1:sims, ate)))
  
return(out)
}
 
simulation = cond.bias()
 
jpeg("cb.jpg", width = 700, height = 480, res=100, units="px")
with(simulation,{
  plot(x = covariate.corr,y = uncond.ate,col=rgb(0,0,1,.5),pch=16,cex=.5,
       xlab = "Correlation between covariate and treatment",
       ylab = "Estimated ATE | True ATE = 1")
  abline(a=1, b=0)
  points(x=covariate.corr,y = cond.ate,col=rgb(1,0,0,.5),pch=16,cex=.5)
  rug(x = uncond.ate,side = 2,col = rgb(0,0,1,.05))
  rug(x = cond.ate,side = 4,col = rgb(1,0,0,.05))
  segments(min(covariate.corr)-.06,      quantile(uncond.ate, c(.025, .975)),min(covariate.corr), quantile(uncond.ate, c(.025, .975)), col = "blue")
  segments(max(covariate.corr),      quantile(cond.ate, c(.025,  .975)),max(covariate.corr)+0.06,  quantile(cond.ate, c(.025, .975)), col = "red")
  legend(.12, y=.8, c("Controlling for Covariate", "No controls"), col=c("red","blue"), pch = 19)
})
dev.off()
```

Here, treatment is randomly assigned which means that we do not expect any particular correlation between the treatment and the covariate. However if we do not control for the covariate then our estimate of the treatment effect is very sensitive to whatever the correlation happens to be. If it is large we will estimate too large a treatment effect; if it is small we will estimate too small a treatment effect. In expectation we will get the right answer, but we don’t expect to get the right answer if we condition our expectations on the correlation between the treatment allocation and the background variable. The figure also shows how introducing a control removes this problem — the expected effect no longer depends on any accidental correlation between treatment and the covariate, and, for this reason, it is more precise to boot.

When do you know if your experiment suffers from imbalance? Many experimenters advocate the practice of presenting a table of descriptive statistics (means and standard deviations on all covariates for each group) and “balance tests.” Including descriptive statistics comparing treatment and control groups is an excellent way to describe the population of your experiment. “Balance tests” calculate the probability that we would have seen a random draw in which treatment and control groups were this balanced or imbalanced. Balance tests are often used as a check to make sure that randomization was done correctly, which can be especially helpful if you are not in control of the random assignment, but remember that a perfectly executed randomization can still fail a balance test 5 percent of the time and yet be correct. Put differently, a balance test does not actually test balance, it tests the hypothesis that there was randomization in the first place (which is something you should know already). An alternative way of capturing the degree of imbalance in your table of descriptive statistics is to look at the size of the deviation in group means along covariates, not the probability that we would have seen this particular draw.

PROBLEMS WITH CODE CHUNK BEFORE THESE PARAGRAPHS