---
title: "10 Things You Need to Know About Multiple Comparisons"
output: html_document
Author: NA
---


Abstract
==
The “Multiple Comparisons Problem” is the problem  that standard statistical procedures can be misleading when researchers conduct a large group of hypothesis. When a research has multiple “bites at the apple,” the chances are that some finding will appear “significant” even when there’s nothing going on.

Classical hypothesis tests assess statistical significance by calculating the probability under a null hypothesis of obtaining estimates as large or larger as the observed estimate. When multiple tests are conducted, however, classical p-values are incorrect — they no longer reflect the true probability under the null.

This guide [^1] will help you guard against drawing false conclusions from your experiments. We focus on the big ideas and provide examples and tools that you can use in R.

[^1]: Originating author: Alex Coppock, 18 Feb 2015. The guide is a live document and subject to updating by EGAP members at any time. Coppock is not responsible for subsequent edits to this guide

Almost every social science experiment faces a multiple comparisons problem
==
Typically, researchers are not interested in just one treatment versus control comparison per experiment. There are three main ways that comparisons proliferate:

1. **Multiple treatment arms.** When an experiment has n treatment arms, there are n(n-1)/2 possible comparisons.
2. **Heterogeneous treatment effects.** Often, we are interested in whether the treatment has different impacts on different subgroups.  For example, a treatment might be more effective for women than for men.
3. **Multiple estimators.** Often, experimenters will apply multiple estimators to the same dataset: for example, difference-in-means and covariate adjustment.
There is of course nothing wrong with employing multiple treatment arms, exploring treatment effect heterogeneity, or using multiple estimators of the treatment effect.  However, these design and analysis choices sometimes require that researchers correct their statistical tests to account for multiple comparisons.

These concerns are especially problematic when making a “family claim,” that is, when you are summarizing a series of results.  For example, a family claim might be that treatments A, B, C, and D had no effect, but treatment E did.  Or, similarly, the treatment had no effect among group 1, group 2, or group 3, but had a strong effect among group 4.

The multiple comparisons problem is related to, but different from, the problem of “fishing”.  Fishing occurs when an unscrupulous analyst conducts many tests but only reports the “interesting” ones. In essence, fishing withholds the necessary information we would need in order to correct for multiple comparisons.

2 Why multiple comparisons are a problem
==
In classical hypothesis testing, the “alpha level” describes how willing the researcher is to make a certain kind of mistake — a so-called Type I error. A Type I error occurs when a researcher falsely concludes that an observed difference is “real,” when in fact there is no difference. In many social science applications, the alpha level, or Type I error rate, is set to 0.05. This means that the researcher is willing to commit a Type I error 5% of the time.

But when we move to the world of multiple comparisons, this simple testing framework is no longer sufficient. We are trying to avoid the scientific error of falsely concluding that an effect is practically or substantively meaningful — but if we apply the classical hypothesis testing framework without addressing multiple comparisons, we are much more likely to commit such an error.

In this guide, we will describe three main approaches for addressing the multiple comparisons problem:

1. p-value adjustments. Statisticians have derived a number of corrections that can guard against multiple comparisons mistakes. As described in the next section, these corrections control either the *Family-Wise Error Rate* (FWER) or the *False Discovery Rate* (FDR).  Most of these adjustments apply a simple formula to a series of “raw” p-values; we will also describe a simulation method that can take account of features of a specific research setting.
2. Pre-analysis plans. These plans are a powerful design-based tool for “calling your shot” so to speak.
3. Replication. If we are concerned that a finding is simply an artifact of sampling variability that we happened to discover because of a blind application of classical hypothesis testing, then the best way to resolve the question is to conduct the experiment again.

3 Don't mix up the FWER and FDR!
==
In the world of multiple testing, the Type I error rate might refer to one of two things: the Family-Wise Error Rate (FWER) or the False Discovery Rate (FDR).

The FWER is the probability of incorrectly rejecting even one null hypothesis. Suppose we have three null hypotheses, all of which are true. When the null hypothesis is true, but we nevertheless reject it in favor of some alternative, we commit a Type I error. If we set alpha (the Type I error rate) to be 0.05, we have a [$1−(1−0.05)^3=14.2%$] chance of rejecting at least one of them. In order to control the FWER (i.e., reduce it from 14.2% back down to 5%), we need to employ a correction. We’ll explore three ways to control the FWER (Bonferroni, Holm, and simulation) in the sections below.

The FDR is subtly different. It is the proportion of false discoveries among all tested (true) null hypotheses. For example, if you are testing 60 hypotheses, a procedure that controls the FDR (at alpha=0.05) will ensure that no more than 3 hypotheses are falsely rejected. The FDR is less stringent than the FWER, but may be more appropriate in some research situations.

4 The Bonferroni correction is too extreme
==
The Bonferroni correction is a commonly-used approach for addressing the multiple comparisons problem, partly because of its simplicity. If you conduct k tests, the target significance level should be alpha/k, or, equivalently, you multiply your p-values by k, and apply the standard alpha level. (The trouble with multiplying the p-values is sometimes you end up with values over one, rendering the interpretation of the p-values incoherent.)

For example, suppose you conduct an experiment that has 3 dependent variables. You conduct three difference-in-means tests that yield the following classical p-values: 0.004, 0.020, and 0.122. If your alpha level is the standard 0.05 threshold, then you would usually declare the first two tests statistically significant and the last test insignificant. The Bonferroni correction, however, adjusts the target p-value to 0.05/3 = 0.016. We then declare only the first test to be statistically significant.

The Bonferroni correction works under the most extreme circumstances, that is, when all k tests are independent from one another. To see how this works, imagine we are testing three true null hypotheses using a classical alpha level of 0.05. Each test, therefore has a 5% chance of yielding the wrong answer that the null hypothesis is false.

But our chances of making at least one mistake are much greater than 5% because we have three chances to get it wrong. As above, this probability is in fact  [$1 - (1 - 0.05)^3 = 14.2%$] . If we use the Bonferroni correction, however, our chances of getting it wrong fall back to our target alpha value:  [$1 - (1 - 0.05/3)^3 \approx 0.05$] .

This correction works in the worst-case scenario that all tests are independent. But in most cases, tests are not independent. That is, if your treatment moves outcome A, it probably moves outcome B too, at least a little. So what tends to happen is, researchers report that their results “withstand” Bonferroni when they are extremely strong, but decry Bonferroni as too extreme when the results are weaker.

Instead of using the Bonferroni correction, you can use the Holm correction.  It is strictly more powerful than Bonferroni, and is valid under the same assumptions. It also controls the FWER. Suppose you have  $m$  p-values.  Order them from smallest to largest. Find the smallest p-value that satisfies this condition: $p_{k}>\frac{α}{m+1−k}$, where $k$ is the p-value’s index. This and all larger p-values are insignificant; all smaller p-values are significant.

Taking our three p-values from above: 0.004, 0.020, and 0.122:
$$0.004<\frac{0.05}{3+1−1}=0.017$$
$$0.020<\frac{0.05}{3+1−2}=0.025$$
$$0.122>\frac{0.05}{3+1−3}=0.050$$

Under the Holm correction, the first two tests are significant, but the last test is not.

5 Control the FDR with Benjamini-Hochberg
==
The Benjamini–Hochberg procedure controls the FDR. Like the Holm correction, you also begin by ordering $m$ p-values. Then you find the largest p-value that satisfies:  $p_{k}≤\frac{k}{m}α$. This test, and all tests with smaller p-values are declared significant.

$$0.004<\frac{1}{3}0.05=0.017$$
$$0.020<\frac{2}{3}0.05=0.033$$
$$0.122>\frac{3}{3}0.05=0.050$$

Using the Benjamini–Hochberg procedure, the first two tests are significant, but the third is not.

6 It’s easy to implement these procedures in R
==
In R, the p.adjust() function contains many of the corrections devised by statisticians to address the multiple comparisons problem. The p.adjust() function is in base R, so no additional packages are required.
```{r}
# Set seed for reproducibility
set.seed(343)
 
# Generate 50 test statistics
# Half are drawn from a normal with mean 0
# The other half are drawn from a normal with mean 3
x <- rnorm(50, mean = c(rep(0, 25), rep(3, 25)))
 
# Obtain 50 p-values
p <- round(2*pnorm(sort(-abs(x))), 3)
 
# Choose alpha level
alpha <- 0.05
 
# Without any corrections
sig <- p < alpha
 
# Conduct three corrections
# and compare to target alpha
bonferroni_sig <- p.adjust(p, "bonferroni") < alpha
holm_sig <- p.adjust(p, "holm") < alpha
BH_sig <- p.adjust(p, "BH") <alpha
```

The results of this simulation are presented in the table and figure below.

Correction Type               | No Correction | Benjamini-Hochberg | Holm | Bonferroni
------------------------------|---------------|--------------------|------|-------------
Statistically Significant     |	25	          |22                  |11    |8
Not Statistically Significant |	25            |28                  |39    |42

```{r, warning=FALSE, echo=FALSE}
library(ggplot2)
# Set seed for reproducibility
set.seed(343)

n_tests <- 50
# Generate 50 test statistics
x <- rnorm(n_tests, mean = c(rep(0, 25), rep(3, 25)))
#x <- rnorm(n_tests, mean = c(rep(0, 50)))

# Obtain 50 p-values
p <- round(2*pnorm(sort(-abs(x))), 3)

# Choose alpha level
alpha <- 0.05

# Without any corrections
sig <- p < alpha

# Conduct three corrections
bonferroni_sig <- p.adjust(p, "bonferroni") < alpha
holm_sig <- p.adjust(p, "holm") < alpha
BH_sig <- p.adjust(p, "BH") <alpha

# Compare corrected and uncorrected
# table(bonferroni_sig, sig)
# table(holm_sig, sig)
# table(BH_sig, sig)

p = rep(p,4)
correction <- rep(c("No Correction", "Benjamini-Hochberg","Bonferroni", "Holm"), each=n_tests)
correction = factor(correction,
                    levels=c("Bonferroni", "Holm", "Benjamini-Hochberg", "No Correction"),
                    ordered=TRUE)
Significant = c(sig, BH_sig, bonferroni_sig, holm_sig)
Significant <- ifelse(Significant, "Statistically Significant", "Not Statistically Signifiant")
Significant <- factor(Significant, levels= c("Statistically Significant", "Not Statistically Signifiant"),
                      ordered=TRUE)

for_gg <- data.frame(p, correction, Significant)
g <- ggplot(for_gg, aes(x=p, y=correction, color=Significant)) + 
  geom_point(alpha = .8, size=3, position=position_jitter(height=.2, width=0)) + 
  xlim(0,.05) + theme_bw() + scale_color_manual(values=c("#45DBC6","#5E56FF")) +
  ylab("") + xlab("Uncorrected p-values") + theme(legend.position="bottom",legend.title=element_blank()) +
  ggtitle("Multiple Comparisons Corrections Compared")
g
```

Of the 25 null hypotheses that would be rejected if no correction were made, the Bonferroni correction only rejects 8, the Holm procedure rejects 11, and the Benjamini–Hochberg procedure rejects 22. Of these three corrections, Bonferroni is the most stringent while Benjamini–Hochberg is the most lenient.

7 A better way to control the FWER is simulation
==
The trouble with the corrections above is that they struggle to address the extent to which the multiple comparisons are correlated with one another. A straightforward method of addressing this problem is simulation under the sharp null hypothesis of no effect for any unit on any dependent variable. Note that this is a **family-wise** sharp null.

If the treatment has no effect at all on anything, then we observe all potential outcomes for all subjects. We can re-randomize the experiment 1000 or more times and conduct all k hypothesis tests each time. We know for sure that all k null hypotheses are true, because the treatment has no effect by construction.

The next step is picking the right threshold value below which results are deemed statistically significant. If alpha is 0.05, we need to find the target p-value that, across all simulations under the sharp null, yields 5% significant hypothesis tests.

Once we have the right threshold value, it’s as easy as comparing the uncorrected p-values to the threshold value — those below the threshold are deemed significant.

```{r, warning=FALSE}
# Control the FWER through simulation
rm(list=ls())
library(mvtnorm)
library(randomizr)

# Helper functions
do_t_test <- function(Y, Z){
  t.test(Y[Z==1], Y[Z==0])$p.value
}

permute_treat <- function(){
  treatment_sim <- complete_ra(n, m=n/2)
  ps_sim <- apply(outcomes, 2, do_t_test, Z = treatment_sim)
  return(ps_sim)
}

threshold_finder<- function(threshold){
  mean(apply(many_ps, 2, x <- function(x) sum(x <= threshold) > 0 ))
}

# Set a seed
set.seed(343)

# Generate correlated outcomes
# Outcomes are unrelated to treatment
# All null hypotheses are true

n <- 1000
k <- 100; r <- .7; s <- 1
sigma <- matrix(s*r, k,k)
diag(sigma) <- s
outcomes <- rmvnorm(n=n, mean=rep(0, k), sigma=sigma)

# Complete Random Assignment
treatment <- complete_ra(n, m=n/2)

# Conduct k hypothesis tests
p_obs <- apply(outcomes, 2, do_t_test, Z = treatment)

# Simulate under the sharp null
many_ps <- replicate(1000, permute_treat(), simplify = TRUE)

# Obtain the Type I error rate for a series of thresholds
thresholds <- seq(0, 0.05, length.out = 1000)
type_I_rate <- sapply(thresholds, threshold_finder)

# Find the largest threshold that yields an alpha type I error rate
target_p_value <- thresholds[max(which(type_I_rate <=0.05))]

# Apply target p_value to observed p_values
sig_simulated <- p_obs <= target_p_value

# Compare to raw p-values
sig <- p_obs <= 0.05
```

The target p-value obtained by the simulation is 0.002 — hypothesis tests with raw p-values below 0.002 are deemed significant.  Compare this with the Bonferroni method, which would require a p-value below 0.05/100 = 0.0005, an order of magnitude smaller.  The closer the correlation of the tests (the parameter “r” in the code above) is to zero, the closer the two methods will be.

The flexibility of the simulation method is both an advantage and a disadvantage. The advantage is that it can accommodate any set of estimation procedures, returning a study-specific correction that will generally be more powerful than other methods to control the FWER. The disadvantage is that it requires the researcher to code up a simulation — there are no prewritten functions that will apply across research contexts.

Here are some guidelines and tips for writing your own simulation.

1. Follow the original random assignment procedure as exactly as possible.  For example, if you block-randomized your experiment, make sure your simulations permute the treatment assignment according to the same blocks.
2. Each simulation should return a set of p-values. (this was accomplished in the permute_treat() function above.)
3. Be sure to count up the number of simulations in which at least one test was deemed significant, not the average number of tests across all simulations deemed significant.

8 You can use this calculator to adjust your p-values
==
This calculator works best in Firefox. To use full-screen, go [here](https://acoppock.shinyapps.io/MC_app).

<iframe height="150" src="https://egap.shinyapps.io/multiple-comparisons-app/" style="border: none; width: 440px; height: 500px;" width="300"></iframe>

9 Creating an index is a way to get a single comparison out of many.
==
Suppose a researcher collected many — say 100 — dependent variables. Imagine that the treatment does not cause significant differences for any of them, but all the point estimates are in the same direction.  If we were to apply a multiple comparisons correction to our 100 tests, our results would get even murkier. However, if we combine all 100 dependent variables into a single index, the resulting dependent variable may in fact exhibit significant differences. You are rewarded in a sense for the fact that many results point in the same direction, even if none of them do strongly.

Unlike the other ways of thinking about the multiple comparisons problem, the index approach can reward researchers for increasing the number of dependent variables.

For details on different approaches to this see [Kling and Liebman](http://www.nber.org/mtopublic/483.pdf) for a strategy that uses a mean effects index (taking averages of standards measures, coded to point in the same direction) and a discussion of a strategy using Seemingly Unrelated Regressions. For an approach that implements direct tests on an index formed from inverse covariance weighting see [Anderson](http://are.berkeley.edu/~mlanderson/pdf/Anderson%202008a.pdf).

10 Use Design Based approaches
==
There are also two design based approaches for thinking about this problem. One is to use pre-analysis plans to describe in advance which comparisons will be made; the number of tests implicated in a Bonferroni or Bonferroni-style correction is specified before any data analysis is conducted. A pre-analysis plan, among its many benefits, helps to clarify the multiple comparisons problem.

A good example is this pre-analysis plan by [Gwyneth McClendon and Rachel Beatty Riedl](http://egap.org/registration/651).  The authors specify 24 individual tests they plan to conduct and state ahead of time that they will employ both the Bonferroni and Benjamini-Hochberg corrections.  The authors do not state beforehand how they will handle a situation in which the corrections disagree; presumably it will be a matter of judgment for both the authors and the readers of their study.

An additional benefit of pre-analysis plans is the ability to specify beforehand what the primary hypothesis is.  There is disagreement among methodologists on this point, but some argue that because the primary hypothesis is not part of a “family claim” that a standardp-value is correct. For example, a researcher might have one primary hypothesis and 10 ancillary hypotheses.  The uncertainty surrounding the primary hypothesis should not depend on the number of ancillary hypotheses.  The advantage of a preanalysis plan is establishing beforehand which of the many hypotheses is the primary one.

A second approach is to turn to replication. Replication is the best guard against drawing false conclusions from a noisy experiment. In particular, replication helps to establish whether heterogeneous effects findings are reliable. Researchers have a large degree of freedom in choosing how to search for heterogeneity — they can explore the entire covariate space searching for interesting interactions. Such a procedure is likely to lead to a large number of false positives. Multiple comparisons corrections such as the ones discussed above might help — but researchers also have discretion as to which correction to apply. Replication addresses this problem directly by measuring the same covariates and looking for differential effects according to the previous experiment’s analysis.

A good example comes from Coppock, Guess, and Ternovski (2015), in which the same experiment was conducted twice.  The experiments measured the effects of two treatments on two dependent variables among four subgroups.  In principle, this leads to 60 possible comparisons per experiment.

* 3 pairwise comparisons between treatment 1, treatment 2, and control.
* 4 subgroups means 4 comparisons against zero plus 6 pairwise comparisons = 10.
* 2 dependent variables
* 3 * 10 * 2 = 60

The figure below shows the results of the experiment and replication.  Study 1 finds that “organization” accounts have smaller treatment effects than females, males, and unknown accounts on the “signed” DV but not the “tweeted” DV.  The uncorrected p-value of the difference between “Organization” and “Male” conditional average treatment effects was 0.00003 for the “follower” treatment and 0.00632 for the “organizer” treatment.  The Bonferroni correction would multiply both p-values by 60, suggesting that the “organizer” treatment does not work significantly differently for organizations versus men.

The replication, however, shows the same pattern of treatment effects: smaller effects for Organization accounts than for others on the “Signed” DV, but similar treatment effects on the “Tweeted” DV.  Any doubt that the different response on the two different dependent variables was due to sampling variability are assuaged by the replication.

![](https://raw.githubusercontent.com/egap/methods-guides/master/multiple-comparisons/multiple-comparisons_fig2.png)
