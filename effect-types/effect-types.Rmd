---
Author: NA
Title: 10 Types of Treatment Effect You Should Know About
output: html_document
---


Abstract
==
This guide [^1] for more formal discussion of independence and the assumptions necessary to estimate causal effects. describes ten distinct types of causal effect researchers can be interested in estimating. As discussed in our guide to causal inference, simple randomization allows one to produce estimates of the average of the unit level causal effects in a sample. This average causal effect or average treatment effect (ATE) is a powerful concept because it is one solution to the problem of not observing all relevant counterfactuals. Yet, it is not the only productive engagement with this problem. In fact, there are many different types of quantities of causal interest. The goal of this guide is to help you choose estimands (a parameter of interest) and estimators (procedures for calculating estimates of those parameters) that are appropriate and meaningful for your data.

[^1]: See Holland (1986) and Angrist and Pischke  Angrist, Joshua, and J√∂rn-Steffen Pischke. 2008. Mostly Harmless Econometrics: An Empiricist‚Äôs Companion. Princeton University Press.

1 Average Treatment Effects
==
We begin by reviewing how, with randomization, a simple difference-of-means provides an unbiased estimate of the ATE. We take extra time to introduce some common statistical concepts and notation used throughout this guide.

First we define a treatment effect for an individual observation (a person, household, city, etc.) as the difference between that unit‚Äôs behavior under treatment $(Y_{i}(1))$ and control $(Y_{i}(0))$:

$$œÑ_{i}=Y_{i}(1)‚àíY_{i}(0)$$

Since we can only observe either $Y_{i}(1)$ or $Y_{i}(0)$ the individual treatment effect is unknowable. Now let $D_{i}$ be an indicator for whether we observe an observation under treatment or control. If treatment is randomly assigned, $D_{i}$ is independent, not only of potential outcomes but also of any covariates (observed and unobserved) that might predict also those outcomes $((Y_{i}(1),Y_{i}(0),X_{i}‚ä•‚ä•D_{i}))$.[^2]

[^2]: See Holland and Angrist & Pischke again for more formal discussion of independence and the assumptions necessary to estimate causal effects.

Suppose our design involves $m$ units under treatment and $N‚àím$ under control. Suppose we were to repeatedly reassign treatment at random many times and each time calculate the difference of means between treated and control groups and then to record this value in a list. The average of the values in that list will be the same as the difference of the means of the true potential outcomes had we observed the full schedule of potential outcomes for all observations.[^3] Another way to say this characteristic of the average treatment effect and the estimator of it, is to say that the difference of observed means is an unbiased estimator of the average causal treatment effect.

[^3]:  That is $(ùîº(Y_{i}(1)‚à£D=1)=ùîº(Y_{i}(1)‚à£D=0)=ùîº(Y_{i}(1))$ and $ùîº(Y_{i}(0)‚à£D=1)=ùîº(Y_{i}(0)‚à£D=0)=ùîº(Y_{i}(0)))$

$$ATE‚â°\frac{1}{N}‚àë^{N}_{i=1}œÑ_{i}=\frac{‚àë^{N}_{1}Y_{i}(1)}{N}‚àí\frac{‚àë^{N}_{1}Y_{i}(0)}{N}$$

And we often estimate the ATE using the observed difference in means:[^4]

[^4]: Estimates are often written with a hat ( $\widehat{ATE}$ ) to reflect the difference between the estimate from our particular sample and the estimand, of target of our estimation that is unobserved. Unless otherwise stated, in this guide we focus on generating sample estimates and subsequently omit this explicit notation for brevity. See Gerber and Green (2012) for concise introduction to this distinction and Imbens and Wooldridge (2007) for a thorough treatment of these concepts.

$$\widehat{ATE} =\frac{‚àë^{m}_{1}Z_{i}Y_{i}}{m}‚àí\frac{‚àë^{N}_{m+1}(1‚àíZ_{i})Y_{i}}{N‚àím}$$

Statistical inference about the estimated ATE requires that we know how it will vary across randomizations. It turns out that we can write the variance of the ATE across randomizations as follows:

$$V(ATE) = \frac{N}{N‚àí1} [\frac{V(Y_{i}(1))}{m}+\frac{V(Y_{i}(0))}{N‚àím}]‚àí\frac{1}{N‚àí1}[V(Y_{i}(1))+V(Y_{i}(0))‚àí2‚àóCov(Y_{i}(1),Y_{i}(0))]$$

and estimate this quantity from the sample estimates of the variance in each group.[^5]

[^5]: The covariance of $Y_{i}(1),Y_{i}(0)$ is impossible to observe but the ‚ÄúNeyman‚Äù estimator of the variance omitting the covariance term provides a conservative (too large) estimate of the true sample variance because we tend to assume that the covariance is positive. Since we are generally worried about minimizing type I error rate (incorrectly rejecting true null hypothesis), we prefer using conservative estimates of the variance. See also Dunning (2010) and Gerber and Green (2012) for justification of the conservative variance estimator.

A linear model regressing the observed outcome $Y_{i}$ on a treatment indicator $D_{i}$ provides a convenient estimator of the ATE (and with some additional adjustments, the variance of the ATE):

$$Y_{i}=Y_{i}(0)‚àó(1‚àíD_{i})+Y_{i}(1)‚àóD_{i}=Œ≤_{0}+Œ≤_{1}D_{i}+u$$

since we can rearrange terms so that $Œ≤_{0}$ estimates the average among control observations $(Y_{i}(0)‚à£D_{i}=0)$ and $Œ≤_{1}$ estimates the differences of means $(Y_{i}(1)‚à£D_{i}=1)‚Äì(Y_{i}(1)‚à£D_{i}=0)$. In the code below, we create a sample of 1,000 observations and randomly assign a treatment Di with a constant unit effect to half of the units. We estimate the ATE using ordinary least squares (OLS) regression to calculate the observed mean difference. Calculating the means in each group and taking their difference would also produce an unbiased estimate of the ATE. Note that the estimated ATE from OLS is unbiased, but the errors in this linear model are assumed to be independent and identically distributed. When our treatment effects both the average value of the outcome and the distribution of responses, this assumption no longer holds and we need to adjust the standard errors from OLS using a Huber-White sandwich estimator to obtain the correct estimates (based on the variance of the ATE) for statistical inference[^6]. Finally, we also demonstrate the unbiasedness of these estimators through simulation.

[^6]: Lin (2013)

```{r}
set., warning=FALSE, echo=FALSEseed(1234) # For replication 
N = 1000 # Population size 
Y0 = runif(N) # Potential outcome under control condition 
Y1 = Y0 + 1 # Potential outcome under treatment condition 
D = sample((1:N)%%2) # Treatment: 1 if treated, 0 otherwise Y = D*Y1 + (1-D)*Y0 # Outcome in population samp = data.frame(D,Y) 

ATE = coef(lm(Y~D,data=samp))[2] #same as with(samp,mean(Y[Z==1])-mean(Y[Z==0])) 

# SATE with Neyman/Randomization Justified Standard Errors 
# which are the same as OLS standard errors when no covariates or blocking 
library(lmtest) 
library(sandwich) 
fit<-lm(Y~D,data=samp) 
coef(summary(fit))["D",1:2]
```