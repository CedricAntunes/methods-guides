---
title: "10 Things to Know About Hypothesis Testing"
author: "Methods Guide Author: Jake Bowers"
bibliography: ../../Research-Group-Bibliography/big.bib
output: 
  html_document:
    number_sections: true
    toc: true
    theme: journal
    extensions: +implicit_figure
    code_folding: hide    
    pandoc_args: [
           --filter, pandoc-crossref
        ]
---

<!-- title: "10 Things to Know About Hypothesis Testing" -->
<!-- author: "Methods Guide Author: Jake Bowers" -->

\newcommand{\bX}{\mathbf{X}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\bSigma}{\boldsymbol{\Sigma}}

<style>
.comment{
background-color: #DBDBDB;
padding: 10px;
border: 1px solid black;
margin-left: 25px;
border-radius: 5px;
color: black;
}
</style>

# Hypothesis tests summarize information in research designs to help people reason about treatment effects

When researchers report that "The estimated average treatment effect is 5
($p=.02$)." they are using a shorthand to say, "Dear reader, in case you were
wondering whether we could distinguish signal from noise in the this
experiment using averages, in fact, we can. The experimental results are **not
consistent** with the idea that the treatment had no effects."

The $p$-value summarizes the ability of a given experiment experiment to
distinguish signal from noise. As we saw in [10 Things You Need to Know about
Statistical
Power](https://egap.org/methods-guides/10-things-you-need-know-about-statistical-power),
whether or not a research design can detect a treatment effect depends not
only on the size of the experimental pool, but also the distribution of the
outcome^[Outcomes with big outliers add noise, outcomes that are mostly 0 have
little signal, blocking or pre-stratification or covariance adjustment can
reduce noise.], the distribution of the treatment, and the substantive strength
of the intervention itself. When a researcher calculates a $p$-value as the
result of a hypothesis test, she is summarizing all of these aspects of a
research design as they bear on a particular claim --- usually a claim that
the treatment had no causal effect.

The rest of this guide explains the pieces of a hypothesis piece by piece:
from hypothesis (the claim that the treatment had no causal effect), to
test-statistic summarizing the observed data (like a difference of means), to
the creation of a probability distribution that allows calculation of a
$p$-value. It also discusses the idea of rejecting (but not accepting) a
hypothesis and touches on the question of what makes for a good hypothesis
test (hint: an ideal hypothesis test should cast doubt on the truth rarely and
distinguish even faint signals from noise).

# In an experiment, a hypothesis is a claim about unobserved causal relationships

We do experiments to make interpretable causal comparisons
[@kinder1993behalf], and we often estimate average causal effects. What does
hypothesis testing have to do with causal inference? In this section we
explain a bit about the distinction between assessing claims about causal
effects versus making best guesses about causal effects.

## A quick overview of the fundamental problem of causal inference and an introduction to some notation

Recall from [Ten Things to Know about Causal Inference](link) that the
counterfactual conceptualization of causality uses the idea of [potential
outcomes](link) to **define** cause and to formalize what we **mean** when we
say "X causes Y" or "Smoking causes cancer" or "Information increases tax
compliance".  Although there are other ways to think about causality
(@brady2008causation), the counterfactual idea suggests that we imagine that
each person, $i$, would pay their taxes, $y_{i}$,  if given
information about the use to which those taxes are put. Write $Z_i=1$ to mean
that information was giving to the person and $Z_i=0$ if no information was
given so that we can write $y_{i,Z_i=1}$ to refer to the amount of taxes paid
by someone given information and $y_{i,Z_i=0}$ to refer to the amount of taxes
paid by someone not given any information in particular. In an actual
experiment, we might [randomize the provision of information to citizens](link
to metaketa II), so some people will have the information and others
will not. We observe the taxes paid by people in both conditions but, for any
one person, we can only observe the taxes that they pay in one of the two
conditions. What does it **mean** when we say "causal effect"? It often means
that the outcome in one condition ($y_{i,Z_i=1}$ written slightly more simply
as $y_{i,1}$) and the outcome in the other condition ($y_{i,Z_i=0}$ or
$y_{i,0}$) *differ* for a given person, such that we would write $y_{i,Z_i=1}
\ne y_{i,Z_i=0}$.

We cannot observe *both* $y_{i,1}$ and $y_{i,0}$ for each person --- if we
gave information about taxes to a person we observe $y_{i,1}$ and so we
cannot observe how they would have acted if they had not been given this
information ($y_{i,0}$). So, we cannot use direct observation to learn about
this counterfactual causal effect and we can only **infer** about it.
@holland1986statistics calls this inability to use direct observation to
learn about counterfactual causality the "fundamental problem of causal
inference". 


## An overview of estimation based approaches to causal inference in randomized experiments. 

There are three main ways that the statistical sciences have engaged with this
problem. That is, when asked, "Does information cause people to pay their
taxes?" we tend to say, "We cannot answer that question directly. However, we can
answer a related question." [Ten Things to Know about Average Treatment
Effects](link) describes an insight that we credit to [Jerzy Neyman](link)
where a scientist can **estimate average causal effects** in a randomized
experiment even if individual causal effects are unobservable. [Judea
Pearl's](link) work on estimating the conditional probability of an outcome
based on a causal model of that outcome is similar to this idea, where a focus
is on averages or aggregates of the individual $y_{i}$'s. That is, those two approaches answer the
fundemental causal question by changing the question focus on averages
or conditional probabilities. A related approach, from [Don Rubin](link) begins by **predicting** the
individual level potential outcomes using background information and a
probability model of $Z_i$ (such that, say, $Z \sim Bernoulli(\pi)$) and a
probability model of the two potential outcomes such that, say,
$(y_{i,1},y_{i,0}) \sim MVN(\bbeta \bX, \bSigma)$. These probability models
can be combined using Bayes Rule to produce posterior distributions for
quantities like the individual level treatment effects or average treatment
effects (see [@imbens2007causal] for more on what they call the Bayesian
Predictive approach to causal inference). So, the predictive approach changes
the fundamental question to focus on differences in predicted potential
outcomes.

## Hypothesis testing is a statistical approach to the fundamental problem of causal inference using claims about the unobserved.

The third approach to this problem changes the question again.
@fisher:1935[Chapter 2] taught us that we can ask the fundamental question
about whether there is a causal effect for a single person, but the answer can
only be in terms of how much information the research design and data provide
about it. That is, one can hypothesize that, for person $i$, the
information made no difference to their outcomes, such that $y_{i,1}=y{i,0}$
or $y_{i,1}=y_{i,0}+\tau_i$ where $\tau_i=0$ for everyone. However, the answer to this
question has to be something like "This research design and dataset provide a
lot of information about this model or idea or hypothesis." or, as above,
"This research design is not consistent with that claim." (See
@rosenbaum2002(Chapter 2), @rosenbaum2010(chapter 2), and
@rosenbaum2017observation, for more details about this approach.)

# The Null Hypothesis of No Effects is a precise statement about potential outcomes

Even if we cannot use direct observation to learn about counterfactual causal
effects, we can still ask questions about them, or make theoretical models
that relate some intervention or treatment, background characteristics, and
potential outcomes. The simplest such model states that the outcome under
treatment would be the same as the outcome under control for all units; that
is, it would say that regardless of background characteristics, or information
given in the experimental treatment condition, each person would pay the same
amount in taxes: $y_{i,1}=y_{i,0}$ for all units $i$. To emphasize the
tentative and theoretical nature of this model, people have called this a
**hypothesis** and often write it as "the sharp null hypothesis."   and write it using this shorthand: $H_0:
y_{i,1}-y{i,0}=\tau_i$ where $\tau_i=0$ for all units $i$.

**Side Note:** Notice that thinking about sharp hypotheses makes us realize that we could
make **other models relating $y_{i,1}$ and $y_{i,0}$** in which the potential
outcomes relate in ways that are not additive or linear, and where the effect need not
be zero or even the same for all units: for example, we could hypothesize that
$\tau_i=\{5,0,-2\}$ 5 for unit 1, 0 for unit 2, and -2 for unit 3 in a 3 unit
experiment. Notice also that writing potential outcomes this way, with the
potential outcome for unit $i$ *only referring to $i$* and not some other
units ($y_{i,Z_i}$), *is part of the model*. That is, the particular model of
$H_0: y_{i,1}=y_{i,0}$ implies that the treatment has no effect on anyone ---
and no effects includes no spillover effects. We
could be a little more specific by writing the potential outcomes like so: The
potential outcome of unit $i$ when it is assigned to treatment, and when all
of the other units are assigned to any other set of treatments
$\bZ_{~i}=\{Z_j,Z_k,\ldots \}$, $y_{i,Z_i=1,\bZ_{~i}}$. See the guide on [10
things to know about experiments with interference, spillover, and networks]
for more on assessing these kinds of hypotheses.

# The weak null hypothesis of no effects is statement about aggregated potential outcomes

An experiment may have some effects on some units but, on average, no effects.
To codify this intuition, researchers can write a null hypothesis
about an *average* of potential outcomes, or some other aggregated summary of
the potential outcomes, rather than about the whole
collection of potential outcomes.

Because most current discussions about causal effects talks about the
*average* of the effects, people write the weak null something like $H_0:
\bar{\tau}=0$ where $\bar{\tau}=(1/N)\sum_{i=1}^N \tau_i$. Again, the
hypothesis is a statement or model of a relationship among only partially
observed potential outcomes. But, here it is about the average of them. One
could, in principle, articulate hypotheses about other aggregates: medians,
percentiles, ratios, trimmed means,etc. However hypothesizing about average
effects simplifies the math and statistics: we know the properties of averages
of independent observations as sample sizes increase, so that we can appeal to
a Central Limit Theorem to describe the distribution of averages in large
samples --- and this, in turn, makes calculating $p$-values quick and easy in
large samples.

# Randomization allows us to use what we observe to test hypotheses about what we do not observe.

Whether one hypothesizes about unit-level effects directly or about averages
of them, we still need to get back to thinking about signal and noise, about
what we observe. A hypothesis only refers to potential outcomes. Above,
assuming no interaction between units, we imagined two potential outcomes per
person, but we only observe one per person. How can we use what we observe to
learn about theoretical models of partially observed quantities? In this
simple experiment, we know that we observe one of the two potential outcomes
per person, depending on which treatment was assigned to that person. So, we
can link the unobserved counterfactual outcomes to an observed outcome ($Y_i$)
using treatment assignment ($Z_i$) like so:

$$ Y_i = Z_i y_{i,1} + (1 - Z_i) y_{i,0} $$ {#eq:identity}

This @eq:identity says that our observed outcome, $Y_i$ (here, amount of
taxes paid by person $i$), is $y_{i,1}$ when the person is assigned to the
treatment group ($Z_i=1$), and $y_{i,0}$ when the person is assigned to the
control group.

How much information does our research design and dataset contain about the
hypothesis? Imagine, for now, the hypothesis that treatment adds 5 to the tax
payments of every single person such that  $H_0: y_{i,1} = y_{i,0} + \tau_i$ where
$\tau_i=5$ for all $i$.

Let us entertain this model for the sake of argument. What would this
hypothesis imply for what we observe? We have the equation relating observed
to unobserved in @eq:identity so, this model or hypothesis would
imply that:

$$ \begin{aligned}  Y_i & = Z_i ( y_{i,0} + \tau_i ) + ( 1 - Z_i) y_{i,0} \\
& = Z_i  y_{i,0} + Z_i \tau_i + y_{i,0} - Z_i y_{i,0} \\
& = Z_i \tau_i  + y_{i,0}
\end{aligned}
$$

What we observe, $Y_i$, would be either $y_{i,0}$ in the control condition,
$Z_i=0$ or $\tau_i + y_{i,0}$ (which would be $5 + y_{i,0}$ in the treatment
condition).

This hypothesis further implies that $y_{i,0} = Y_i - Z_i \tau_i$ or $y_{i,0}
= Y_i - Z_i 5$. If we subtracted  5 from each observed response in the
treatment condition, then our hypothesis implies that we would observe
$y_{i,0}$ for everyone. That is, by subtracting  5, we would make the
control and the treatment group equivalent in observed outcomes. **This logic gives us an
observable implication of the hypothesis or model of the relationship between
potential outcomes.**

**The sharp null hypothesis of no effects** specifies that $\tau_i=0$ for all
$i$. And this in turn implies that $y_{i,0} = Y_i - Z_i \tau_i = Y_i$. That
is, it implies that what we observe, $Y_i$, is what we would observe if every
unit were assigned to the control condition. And the implication then, is that
we should see no differences between the treated and control groups in their
observable outcomes.

**The weak null hypothesis of no effects** specifies that
$\bar{\tau}=\bar{y}_{1} - \bar{y}_0 = 0$, and
we can write a similar identity linking means of potential outcomes to means
of observed outcomes in different treatment conditions. 

# Test statistics summarize the relationship between observed outcomes and treatment assignment.

Given a hypothesis and a mapping from unobserved to observed outcomes, the
next ingredient in a hypothesis test is a test statistic. A test statistic
summarizes the relationship between treatment and observed outcomes using a
single number. In general, we would like our test statistics to take on larger
values the larger the treatment effect. The code below, for example, shows are two such test
statistics using an example experiment with 6 units randomized into two
groups. 
```{r echo=TRUE}
## Create some data, 
##  y0 is potential outcome to control
y0 <- c(0,1,3,4,50,60)
## Different treatment effects
tau <- c(1,1,0,0,2,2)
## y1 is potential outcome to treatment
y1 <- y0 + tau
## Z is treatment assignment
Z <- c(1,0,1,0,1,0)
## Y is observed outcomes
Y <- Z*y1 + (1-Z)*y0

trueATE <- mean(y1) - mean(y0)

## The mean difference test statistic
meanTZ <- function(y,z){ 
	mean(y*z) - mean(y*(1-z))
}

## The difference of mean ranks test statistic
meanrankTZ <- function(y,z){
	ranky <- rank(y)
	mean(ranky*z) - mean(ranky*(1-z))
}
#meanTZ(y=Y,z=Z)
#meanrankTZ(y=Y,z=Z)
```

The first test statistic is the mean difference (`meanTZ`) and returns an
observed value of `r meanTZ(y=Y,z=Z)` and the
second is the mean difference of the rank-transformed outcomes (`meanrankTZ`),
which returns a value of `r meanrankTZ(y=Y,z=Z)`.
One could also use versions of these test statistic standarized by their estimated
standard error (see @chung2013exact for an argument in favor of this test
statistic).


# $p$-values encode how much information a research design and test statistic tell us about the hypothesis. Hypothesis tests require distributions of the test statistic under the hypothesis.

Given a claim about the possible results of the experiment (i.e. an hypothesis) and
a way to summarize the observed data as it bears on the hypothesis (i.e. a test
statistic that should get bigger as the results diverge from the hypothesis
as explained above), we now want to move beyond description of the observed
data to reflecting how much natural variability we would expect to see in the
test statistic given the research design (to get us back to the question of
signal and noise).

How much evidence we have about a hypothesis depends on the design of the
study. A large experiment, for example, should have more information about
this hypotehsis than a small one. So, what do we mean by evidence against the
hypothesis. How would we formalize or summarize this, so that larger
experiments tend to reveal more and small experiments tend to reveal less
information? 

One answer to this question is to refer to the thought experiment of repeating
the study. Imagine, for the sake of argument, that the hypothesis was correct.
If we repeated the study and calculated the test statistic we would receive a
number – this number would reflect the outcome of the experiment *under the
hypothesis*. Now, imagine repeating the hypothetical experiment many times,
recalculating the test statistic each time. The distribution of the test
statistics would  then tell us all of the test statistic could have occurred
if the null hypothesis were true. If the test statistic is a sum or mean
then, in a large experiment, we know that the distribution of those numbers
will be more closely concentrated around the focal hypothesized value (say,
t(Z,y_0)$) than in a small experiment.

When we compare what we actually observe, $t(z,Y)$ under the null to what we
could have observed under the null (the distribution), we learn about how
extreme or weird or surprising is our given study. And we encode this
extremity with a $p$-value. 

Notice that the $p$-value does not tell us about the probability associated
with the observed data. The observed data is observed. The probability arises
from the hypothetical, but possible, repetition of the experiment itself, the
test statistic, and the hypothesis. The one-tailed p-value is the probability
of seeing a value of our test statistic as great or greater than we actually
observed considering, for the sake of argument, a given hypothesis.

## An example.

INSERT EXAMPLE USING THE TEST STATISTICS ABOVE.


# In simple hypothesis tests, we can reject a null hypothesis but not to accept it.

Sometimes people want to make a decision using the $p$-value. Remember that a
$p$-value uses a test statistic and the idea of repeating the experiment to
quantifies information from the research design about a hypothesis. It is the
design, test statistic function and hypothesis which generates a probability
distribution. And it is the actual data, design, and test statistic function
that creates a single observed value.

So, the $p$-value just tells us how extreme the observed result is from the
perspective of the hypothesis. Or, how inconsistent it is with hypothesis.
What if we want to make a decision? It turns out that we can make decisions
using a $p$-value if we are willing to accept a certain amount of error. Say,
for example, we see a one-tailed $p=.01$, this would mean that, in the
hypothetical world of the null hypothesis, in only 1 in
100 experiments would we see a result as large or larger than our actual
result. We might be tempted to say, that our observed result is so strange
that we want to act as if the null were false. This would be ok --- after all
a $p$-value along cannot control the behavior of an adult human --- but the
human has to know that in 1/100 cases where the null is true, we would still
see this result. That is, if we rejected the null, or acted as if the null
were false, based on a small $p$-value we would be making an error --- a false
rejection, sometimes called a false positive because the null hypothesis is so
often zero and the desired effect (say, in medical trials) is so often coded
as positive.

Say we were happy to make 1 false positive error or false rejection in every
20 experiments. In that case, we should also be happy to reject a null hypothesis if we saw a $p
\le 1/20$ or  $p \le .05$, calling $p$-values smaller than .05 signals of
inconsistency with the null hypothesis should only lead us to err in 5% of
experiments like this one.

Notice that this kind of reasoning --- realizing that decision making involves
error leads us to easily justify certain decisions (like saying that small
$p$-values justify calling the observed experiment inconsistent with the
hypothesis, that is, rejecting the null) but not others: It would not be
reasonable to say that a large $p$-value means that the null is true --- the
null is just a model or a claim. (develop this a bit more)

## What does it mean to reject a null hypothesis?

Notice that a $p=.01$ only reflects extremity of the observed data compared
with the hypothesis. So, it can cast doubt on whether the specific hypothesis
is a good model of the observed data.

## What does it mean to **not reject** a null hypothesis?

Notice that a $p=.50$ only reflects extremity of the observed data compared
with the hypothesis --- but the observed data, in this case, do not look
extreme but common from the perspective of the null hypothesis.


# Once you are using $p$-values to rejecting a hypothesis, then you can make errors such as falsely rejecting a true null

## False Positive Errors

## Familywise Errors  and FDR: multiple comparisons: multiple outcomes, and multi armed treatments/factorial experiments.

# You can also fail to reject a true null: this is basically the power of a test

Once you have a test with a controlled false positive rate, you can ask
another question: a good test is one that easily distinguishes signal from
noise. (connect to the Power guide).

# What else to know about hypothesis tests.

 - A $100\alpha$\% confidence interval can be defined as the range of hypotheses where all of the $p$-values are greater than or equal to $\alpha$. This is called inverting the hypothesis test. (@rosenbaum2010design}
 - A point estimate based on hypothesis testing is called a Hodges-Lehmann point estimate. (@rosenbaum1993hlp,@hodges1963elb)
 - A set of hypothesis tests can be combined into one single hypothesis test
   (@hansen:bowers:2008,@caughey2017nonparametric)
 - In equivalence testing, one can hypothesize that two test-statistics are equivalent (i.e. the treatment group is the same as the control group) rather than
   only about one test-statistic (the difference between the two groups is
   zero) {@hartman2018equivalence}
 - Since a hypothesis test is a model of potential outcomes, one can use
   hypothesis testing to learn about complex models, such as models of
   spillover and propagation of treatment effects across networks (@bowers2013reasoning, @bowers2016research, @bowers2018models)



# References

Lehmann and Romano (2005, Chapters 3 and 5).

