---
bibliography: sampling.bib
output: 
  html_document:
    toc: true
    theme: journal
---

<!-- title: "10 Things to Know About Sampling" -->
<!-- author: "Methods Guide Author: Anna M. Wilke" -->

[Add introduction]
- define target population, elements (observation unit) and sample in intro (see p. 3 in Lohr)

1.	What is random sampling?
==

Random sampling, also referred to as probability sampling, is one way of selecting a sample of elements from a target population. The key characteristic of random sampling is that each element in the population has a *known* and *non-zero* probability of being included in the sample. This goal is achieved by using some randomization mechanism -- be it a coin flip, a random number table, or a computer program -- to decide which elements of the population are going to be part of the sample. 

To make matters concrete, suppose that our target population consists of six elements:
$$U = \{1, 2, 3, 4, 5, 6\}$$
Imagine that we would like to draw a random sample of four elements from this population. There are 15 possible samples of this size that we may draw:
$$
\begin{aligned}
 S_1 & = \{1,2,3,4\} & S_6 & = \{1,2,5,6\} & S_{11} & = \{2,3,4,5\}\\
 S_2 & = \{1,2,3,5\} & S_7 & = \{1,3,4,5\} & S_{12} & = \{2,3,4,6\}\\
 S_3 & = \{1,2,3,6\} & S_8 & = \{1,3,4,6\} & S_{13 }& = \{2,3,5,6\}\\
 S_4 & = \{1,2,4,5\} & S_9 & = \{1,3,5,6\} & S_{14} & = \{2,4,5,6\}\\
 S_5 & = \{1,2,4,6\} & S_{10} & = \{1,4,5,6\} & S_{15} &  = \{3,4,5,6\}\\
\end{aligned}
$$
One selection procedure that would satisfy the definition of random sampling is to use a statistical software to generate a random integer between 1 and 15 and to select the corresponding sample. It is easy to check that each element is included in 10 of the 15 samples. Using this procedure, each element thus has a positive probability of $\frac{10}{15} = \frac{2}{3}$ of being sampled.  Suppose we instead ask a colleague to arbitrarily choose four integers between one and six and include the corresponding elements in the sample. This procedure would not satisfy the definition of random sampling. Neither would we know how often our colleague would pick each element if we asked her to repeat the process many times; nor can we be sure that each element would be picked at some point with positive probability. Note that the selection probabilities need not be the same for all elements for a sampling procedure to qualify as random. For example, we could choose a procedure in which the first unit has a greater probability of being sampled than all other units. What matters is that each element's probability of being sampled is *known* to the researcher. 

The core benefit of random sampling is that it makes it possible to draw inferences about the population based on the characteristics of one realized sample. For example, suppose we are interested in estimating the average income of the population and the population distribution of incomes is as follows:

```{r, echo = FALSE}
id = c(1,2,3,4,5,6)
income = c(25,18,33, 22,50, 45)

data <- data.frame(Income = income)
data <- t(data)
rownames(data) <- "Income (in 1000 USD)"
knitr::kable(data, col.names = id, row.names = T)

```

In this example, the average income in the population (in thousands of USD)  is $$\mu = \frac{25 + 18 + 33 + 22 + 50 + 45}{6} \approx 32.$$ Suppose that we draw our random sample of four units and use the average income in the sample as an estimate of the average income in the population. Since we are basing our estimation on a sample and not on the entire population, our estimate will be prone to error. If we were to select the first of the 15 samples listed above, for example, our estimate would be $$\hat{\mu} = \frac{25 + 18 + 33 + 22}{4} = 25.$$ The difference between the true parameter and our estimate $\mu - \hat{\mu} = 32 - 25 = 7$ is referred to as *sampling error*. Yet, even though estimates based on a single realized sample are subject to sampling error, the process of random sampling (in combination with the chosen estimation procedure) can ensure that our estimates are correct on average across all possible random samples.

```{r, echo = FALSE,message=F, error=F,warning=F}
library(utils)
library(ggplot2)
library(dplyr)

possible_samples <- combn(c(1:6), 4)

estimates <- sapply(1:ncol(possible_samples), function(x) mean(income[possible_samples[,x]]))

plot_data <- data.frame(estimates = estimates)

  

ggplot(plot_data, aes(x = estimates)) + 
  geom_histogram(bins = 6, fill = "white", color = "black") + 
  theme_bw() + xlab("Sample  estimates of average income") + 
  ylab("Count") +
  geom_vline(aes(xintercept=mean(estimates)),
            color="blue", linetype="dashed", size=1)
  

```

The figure above plots the distribution of income estimates that results from the 15 possible samples. This distribution is called the *sampling distribution*. As shown by the blue dashed line, the average estimate across all possible samples coincides with the true population parameter of $\sim 32$. In other words, if we were to  repeat the sampling procedure many times and estimate the average income every time, we would be correct on average. Of course, this fact alone tells us little about the extent to which we should trust an estimate that is based on only one realized sample. A key advantage of random sampling is, however, that it allows us (in most cases) to obtain estimates of the variability of the sampling distribution and thus of the precision of our estimates from a single sample.^[While only random sampling allows for the characterization of sampling uncertainty, random sampling does not guarantee the ability to obtain uncertainty estimates [see @kish1965survey p.24 for a discussion]. A design that samples only one cluster at random, for example, would not allow for the construction of standard errors and confidence intervals.] Typically, these uncertainty estimates take the form of standard errors that can be used to construct confidence intervals. 

For example, we may calculate a 95\% confidence interval for the average income. Even though we would not be certain that our particular sample estimate falls into the confidence interval, we would know that, were we to repeat the process of random sampling and confidence interval creation many times, 95\% of confidence intervals would contain the true population parameter. 

2. What distinguishes random sampling and random assignment?
==

The core feature of both random sampling and random assignment is that units are selected from some larger pool with known probabilities between zero and one. Yet, there are several key differences. In random sampling, units or elements are selected at random from a population in order to infer characteristics of the population from the sample. Random assignment, on the other hand, is used in experiments to allocate  units from a pool of experimental subjects to experimental conditions - in the easiest case to a treatment and a control group. The goal of random assignment is to draw inferences about causal effects, typically the average causal effect of the treatment on some outcome (see ["10 Strategies for Figuring out if X Caused Y"](https://egap.org/resource/10-strategies-for-figuring-out-if-x-caused-y/) for details).^[On a deeper level, however, random assignment has much in common of with random sampling. The process of randomly assigning subjects to a treatment group can be thought of as a process of drawing a random sample of treated potential outcomes in order to estimate the average treated potential outcome in the entire experimental subject pool [@splawa1990application]. ]
 

The subject pool of an experiment may or may not be a random sample from a larger population. In many cases, experiments rely on convenience samples. Combining random sampling and random assignment, however, has important benefits. Suppose we first draw a random sample of subjects from a population and then randomly assign some of these subjects to receive a treatment. That we used random assignment to allocate subjects to experimental conditions enables us to obtain an unbiased estimate of the average treatment effect among the subjects in our sample. That we randomly sampled our subjects from the population means that we can use our estimate of the average treatment effect in the sample to draw inferences about the average treatment effect in the population. In other words, experiments that are based on random samples offer straightforward ways to learn not only about sample average treatment effects (SATEs) but also about population average treatment effects (PATEs). 

Another advantage of randomly sampling experimental subjects is that subjects who have not been sampled can serve as additional control units. To illustrate, consider again the population of six units from the stylized example above. Suppose we would like to conduct an experiment, but  we only have research funds to measure outcomes for four units. Hence, we randomly sample, say, units 1, 2, 3 and 5, and then randomly assign units 2 and 5 to treatment. In principle, the control group of our experiment consists of units 1 and 3. Units 4 and 6, however, by  virtue of not being sampled, have also been chosen to not receive the treatment through a random process. Hence, they can be used as additional control group units should more funds for outcome measurement become available at a later stage. 



3. How to obtain a random sample?
==

Obtaining a random sample is rarely as straightforward as the above description makes it seem. In practice, the *survey population* -- the set of elements for which data can realistically be collected -- is often more restrictive than the *target population*. For example, the target population may consist of all adults who reside in a given country, but it may be infeasible to sample certain groups like prisoners or residents of areas with a difficult security situation.

The first step in random sampling is to acquire a *sampling frame*, a set of materials that are used to identify the elements of the target  population. In its simplest form, a sampling frame may be a list of elements. Sampling frames are rarely perfect and tend to be one the sources of differences between the target and survey population. Sampling frames may suffer from both undercoverage (elements that are part of the target population are missing from the sampling frame) and overcoverage (the sampling frame includes elements that are not part of the population). See @kish1965survey[chapter 2] and @groves2011survey[chapter 3] for more on sampling frame issues and how to remedy them.

Depending in part on the nature of the sampling frame, there are different ways to randomly sample elements. Many of these have analogues among the ways in which subjects can be randomly assigned to experimental conditions (see ["10 Things to Know About Randomization"](https://egap.org/resource/10-things-to-know-about-randomization/)): 

## Simple Random Sampling

Simple random sampling is the most basic survey design. In this design, each sample of size $n$ and hence each element has the same probability of being sampled. One way of drawing a simple random sample of size $n$ from a sampling frame with $N$ elements is to enumerate all possible samples of size $n$ and randomly select one of them. This is the procedure described in the example above. However, this approach tends to be impractical in real world application, since actual populations will be much larger than $N=6$ and the number of possible samples will thus be vast. An alternative procedure is to number all elements from 1 to $N$, to generate $n$ random numbers and to select the corresponding elements.


## Stratification 

Suppose we know upfront that the characteristic in which we are interested varies across sub-populations. For example, if we aim to estimate average income, we may suspect that men earn more than women. If we draw a simple random sample, it is possible that we end up with a sample that contains way more women than men. In this case, our average income estimate is likely to be misleading. A way to guard against this possibility is to divide the population into subgroups, also called *strata*, and draw an independent random sample in each stratum. For example, we may draw an independent random sample of women and and one of men. This procedure fixes the proportion of women and men in the sample, thereby guarding against "bad" samples and improving the pricision of our estimates. Being able to ensure that the sample contains enough elements from a particular sub-population is also helpful if estimates among the sub-population are of independent interest. If we would like to estimate the gender pay gap, for example, we need a sample that contains enough men and women to obtain sufficiently precise estimates of each group's average income. 


## Clustering

Suppose we would like to estimate the average income among residents of a city. Our sample frame may not identify individual residents but we may  have access to a list of households. Instead of directly sampling residents, we can randomly select households and interview all members of the selected households. In this case, the *primary sampling units* -- the units that can be selected -- differ from the elements on which measurements are taken. Households serve as PSUs or *clusters*, while household members serve as elements.  Cluster sampling has one core downside. To the degree that elements within clusters are similar, cluster sampling will lead to a loss in precision. For example, members of the same household may have similar incomes or views.  Nonetheless, cluster sampling may be necessary if sampling frames of individual elements cannnot be obtained.  That said, cluster sampling may also save survey costs. For example, suppose we would like to estimate household-level income. Simple random sampling may lead to a sample of households that is dispersed throughout the city, which increases transportation costs. Instead, we may select entire blocks and interview all households within the selected blocks. Doing so may make it possible to interview more hosueholds with a smaller budget. Whether or not the gain in precision from a bigger sample size outweights the loss in precision from clustering will depend on the degree to which households within the same block are similar in terms of income. 


## Multi-stage sampling

Instead of sampling all elements in a cluster, one may draw a sub-sample of elements. For example, instead of interviewing all members, one could sample two members in each sampled household. Additional stages can be added to this approach. For example, we could first draw a sample of blocks, then a sub-sample of households within each sampled block and finally a sub-sample of household members within each sampled household. In this example, households would be referred to as *secondary sampling units* and household members as *tertiary sampling units*. One advantage of multi-stage sampling is that it allows researchers to navigate the trade-off between precision and cost-effectiveness presented by the choice between cluster sampling and simple random sampling. Increasing the number of clusters and sub-sampling fewer elements per cluster can yield a more diverse sample and hence reduce sampling variability. Yet, doing so may also increase survey costs. 

4. What to do if there does not exist an obvious sampling frame?
==

In many cases, an obvious sampling frame in the form of a list of elements or clusters of elements does not exist, especially when it comes to survey research in developing contexts. In practice, researchers therefore often rely on some variant of multi-stage *area sampling*. For example, they may first randomly sample districts, then randomly select villages within districts, then households within villages and finally individuals within households. Lists of administrative units often exist at higher levels of aggregation, but the problem of non-existent lists of appropriate sampling units kicks in for the more fine-grained stages of the process. The advantage of the multi-stage approach is that such lists only have to be created for units that have actually been sampled. Listing all households in a country, for example, may be infeasible. Listing all households in villages that have entered the sample at the previous stage, however, may be realistic. 

*Create your own sampling units.* 


Irrespective of whether the next larger sampling unit was pre-existing or self-created, lists of households within those sampling units rarely exist. Two popular solutions to this problem are to use a *random walk procedure* or to conduct a *household listing*.

*Random walk procedure.* Enumerators are given (possibly randomly selected) starting points and are instructed to follow a particular walk pattern, e.g. "walk to the right and conduct an interview in every fifth dwelling." The main advantage of this approach is its cost effectiveness. No listing of households beyond those that enter the sample is required. Yet, a random walk procedure may not yield a particularly good approximation to a random sample. First, even though it is often treated as a way to select a simple random sample, a random walk procedure in fact introduces geographic clustering. The walk pattern above, for example, implies that neighboring houses can never both be sampled and that once a particular household is sampled, the one that lives five houses down the road will be sampled as well. See @lohr2009sampling[chapter 5.5] for more on the consequences of this kind of clustering. Second, even if starting points are, say, randomly chosen geo-coordinates, the probability of being sampled may vary across hosueholds due to geographic pacticularities, such as the street layout, density of dwellings etc. If selection  probabilities are not known to the researcher but correlated with the population characteristic of interest, bias will result. Finally, a random walk procedure is difficult to implement in practice. One problem is that enumerators typically select households and conduct interviews in one step. This procedure increases the incentives of enumerators to informally replace households that are reluctant or unavailable -- a practice that can only be detected through intensive monitoring. 

*Household listing.* The more principled yet also more expensive option is to create a list of all households in the sampling unit so that households can be randomly selected from this list prior to the start of the survey. A listing exercise can be expensive, but is also more likely to allow the researcher to approximate a  random sample. Sampling from a full list of households avoids problems of geographic clustering and unknown sampling probabilities. Moreover, separating the listing from the enumeration step makes it easier to guard against unwarrented replacements. At the listing stage, enumerators have fewer incentives to skip houses, since it is not yet clear whether a household will cooeprate at the survey stage. That a record of all households in and outside the sample exists prior to the start of survey activities can also make it easier to  detect instances in which enumerators interviewed the incorrect household. Finally, possibilities for cost-sharing exist, since lists may be re-used for random sampling in the future, at least in places where mobility is low. 

In both options, the final step is usually a listing of household members. 

        
5. What to consider when putting a sampling strategy into practice?
==

Putting andom sampling into practice is extremely difficult. Many unforeseen problems can occur and these are often context-specific. Hence, pre-testing one's sampling strategy is at least as if not more important than pre-resting one's questionnaire. Here are some examples of difficulties that one may be able to discover and address during a pre-test:

  - Definitions of "dwelling" or "household" can vary across contexts. Important to clarify the definition and find ways to deal with edge cases.
  - Familiarity with and ability to read maps may vary across contexts. When working with satellite imagery, it may be important to highlight landmarks. 
  - When sampling household members, these are often identified by gender and age (first invented by Kish 1965). In some contexts, households members may not know their age or birthday, in which case some other marker should be used.

- Keeping it simple is  an advantage. Sophisticated sampling strategies may not be worth it if they cannot be implemented even though they may bring some gains in precision. Clear instructions to survey staff are very important. 

- It is often helpful to separate out tasks, e.g. at IPA Uganda it is common practice that survey teams comprise ``mobilizers'' who are solely responsible for listing households, drawing the sample of households, sampling household members and making appointments. This is helpful because these sampling-related tasks can be complex and may be too much to bear for enumerators who also have to be familiar with the questionnaire and interviewing techniques. 

6. How does one's sampling strategy affect one's analysis strategy?
==

- Experimentalists are accustomed to the notion that one's random assignment strategy affects one's analysis strategy. The same goes for one's sampling strategy. Two key examples:

A) Weight by the inverse of an element's probability of being sampled if sampling probabilities vary across elements.
  - Arises commonly if one household member is sampled in every household but households are of varying size. 
  - E.g. weighted sample mean as estimator of population mean
  - Same applies to treatment effect estimation in randomized experiments that are based on random samples where sampling probabilities vary across subjects. Estimating the PATE requires weighting by sampling probabilities. Yet, doing so also has costs in terms of precision. Bias will only arise if sampling weights are correlated with treatment effects. Not always clear that weighting is the best strategy. Cite Miratrix et al. 2018 who show that weighting does little to results of survey experiment and suggest post-stratifying on weights in order to reduce loss in precision.
  
B) Adjust variance estimates for clustering. 
  - in treatment effect estimation in randomized experiments that are based on random samples, we may want to cluster at a level higher than that of the level of assignment when sampling was clustered at higher levels (Abadie et al. 2017 WHEN SHOULD YOU ADJUST STANDARD ERRORS FOR CLUSTERING? )

7. How to take into account  variability from both random sampling and random assignment?
==

- A more general point is that if the units in an experiment are a random sample of a larger population, then our realized estimate may deviate from the true PATE for two reasons:
  - 1)  we have drawn one of many possible samples instead of performing our analysis on the entire population,
  - 2) we cannot observe all potential outcomes and have therefore chosen one of many possible ways to assign units to treatment and control. 
  - if we want to make inferences to population quantities, our uncertainty estimates should reflect both kinds of variability
- Randomization inference only considers one kind of variability 
- Combining randomization inference with bootstrapping? [IS THAT A THING?]
-  Aronow/Green/Lee: SHARP BOUNDS ON THE VARIANCE IN RANDOMIZED EXPERIMENTS:
  - Standard Neyman standard errors are conservative if we only presume variation from random assignment, can also be justified if we sample from an infinite superpopulation
  - this paper provides bounds for the case of sampling from a finite population

8. How can sampling help with problems of non-response?
==

- Describe double sampling approach (Coppock et al. 2016)

9. How to sample difficult-to-reach populations?
==

- Difficult to draw a random sample of populations that are rare
- Inferential target is often the prevalence of some characteristic among the target population or the size of the difficult-to-reach population itself
- Respondent-driven-sampling as one solution to this problem
- Procedure: Start with a non-random initial sample, hand out coupons through which respondents in initial sample can recruit more respondents
- The ideas is that conducting many waves reduces the influence of the initial non-random sample
- Still does not yield a probability sample because probabilities of selection are not known
- Various approximations exist, most common one thinks of sampling process as a random walk. Implies that  unconditional probabilities of selection are proportional to number of network ties. The latter can be elicited and  used for weighting.
- This model is known to be false in many ways (e.g. presumes sampling with replacement). Gile et al. 2018 describe a range of subsequent approaches that seek to relax the problematic assumptions. 

10. What else to consider when designing a sampling strategy for a randomized experiment?
==

- A random sample is often not possible and experiments rely on convenience samples. 
- Nonetheless, there are important considerations to take into account when designing a sampling strategy.
- Most importantly, maintain symmetry across treatment and control. For example, suppose an experiment evaluates the effect of community meetings. Sampling respondents from a radius around the community hall where the meeting took place sounds like a good strategy. Yet, it only is if there are community halls in control group villages in which the community meeting would have taken place if the village had been assigned to treatment. Otherwise, the same sampling strategy cannot be implemented in treatment and control which may lead to an exclusion restriction violation. 
  - There may be more subtle versions of this. Generally, great to sample units prior to assignment. When sampling units post assignment, make sure that sampling is unlikely to be affected by treatment. 
- You may want to sample in a way to avoid spillovers - e.g. if households are the unit of randomization, you may want to impose a distance constraint to avoid spillover. 



References
==
