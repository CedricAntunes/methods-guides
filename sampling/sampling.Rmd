---
bibliography: sampling.bib
output: 
  html_document:
    toc: true
    theme: journal
---

<!-- title: "10 Things to Know About Sampling" -->
<!-- author: "Methods Guide Author: Anna M. Wilke" -->

[Add introduction]
- define target population, elements (observation unit) and sample in intro (see p. 3 in Lohr)

1.	What is random sampling?
==

Random sampling, also referred to as probability sampling, is one way of selecting a sample of elements from a target population. The key characteristic of random sampling is that each element in the population has a *known* and *non-zero* probability of being included in the sample. This goal is achieved by using some randomization mechanism -- be it a coin flip, a random number table, or a computer program -- to decide which elements of the population are going to be part of the sample. 

To make matters concrete, suppose that our target population consists of six elements:
$$U = \{1, 2, 3, 4, 5, 6\}$$
Imagine that we would like to draw a random sample of four elements from this population. There are 15 possible samples of this size that we may draw:
$$
\begin{aligned}
 S_1 & = \{1,2,3,4\} & S_6 & = \{1,2,5,6\} & S_{11} & = \{2,3,4,5\}\\
 S_2 & = \{1,2,3,5\} & S_7 & = \{1,3,4,5\} & S_{12} & = \{2,3,4,6\}\\
 S_3 & = \{1,2,3,6\} & S_8 & = \{1,3,4,6\} & S_{13 }& = \{2,3,5,6\}\\
 S_4 & = \{1,2,4,5\} & S_9 & = \{1,3,5,6\} & S_{14} & = \{2,4,5,6\}\\
 S_5 & = \{1,2,4,6\} & S_{10} & = \{1,4,5,6\} & S_{15} &  = \{3,4,5,6\}\\
\end{aligned}
$$
One selection procedure that would satisfy the definition of random sampling is to use a statistical software to generate a random integer between 1 and 15 and to select the corresponding sample. It is easy to check that each element is included in 10 of the 15 samples. Using this procedure, each element thus has a positive probability of $\frac{10}{15} = \frac{2}{3}$ of being sampled.  Suppose we instead ask a colleague to arbitrarily choose four integers between one and six and include the corresponding elements in the sample. This procedure would not satisfy the definition of random sampling. Neither would we know how often our colleague would pick each element if we asked her to repeat the process many times; nor can we be sure that each element would be picked at some point with positive probability. Note that the selection probabilities need not be the same for all elements for a sampling procedure to qualify as random. For example, we could choose a procedure in which the first unit has a greater probability of being sampled than all other units. What matters is that each element's probability of being sampled is *known* to the researcher. 

The core benefit of random sampling is that it makes it possible to draw inferences about the population based on the characteristics of one realized sample. For example, suppose we are interested in estimating the average income of the population and the population distribution of incomes is as follows:

```{r, echo = FALSE}
id = c(1,2,3,4,5,6)
income = c(25,18,33, 22,50, 45)

data <- data.frame(Income = income)
data <- t(data)
rownames(data) <- "Income (in 1000 USD)"
knitr::kable(data, col.names = id, row.names = T)

```

In this example, the average income in the population (in thousands of USD)  is $$\mu = \frac{25 + 18 + 33 + 22 + 50 + 45}{6} \approx 32.$$ Suppose that we draw our random sample of four units and use the average income in the sample as an estimate of the average income in the population. Since we are basing our estimation on a sample and not on the entire population, our estimate will be prone to error. If we were to select the first of the 15 samples listed above, for example, our estimate would be $$\hat{\mu} = \frac{25 + 18 + 33 + 22}{4} = 25.$$ The difference between the true parameter and our estimate $\mu - \hat{\mu} = 32 - 25 = 7$ is referred to as *sampling error*. Yet, even though estimates based on a single realized sample are subject to sampling error, the process of random sampling (in combination with the chosen estimation procedure) can ensure that our estimates are correct on average across all possible random samples.

```{r, echo = FALSE,message=F, error=F,warning=F}
library(utils)
library(ggplot2)
library(dplyr)

possible_samples <- combn(c(1:6), 4)

estimates <- sapply(1:ncol(possible_samples), function(x) mean(income[possible_samples[,x]]))

plot_data <- data.frame(estimates = estimates)

  

ggplot(plot_data, aes(x = estimates)) + 
  geom_histogram(bins = 6, fill = "white", color = "black") + 
  theme_bw() + xlab("Sample  estimates of average income") + 
  ylab("Count") +
  geom_vline(aes(xintercept=mean(estimates)),
            color="blue", linetype="dashed", size=1)
  

```

The figure above plots the distribution of income estimates that results from the 15 possible samples. This distribution is called the *sampling distribution*. As shown by the blue dashed line, the average estimate across all possible samples coincides with the true population parameter of $\sim 32$. In other words, if we were to  repeat the sampling procedure many times and estimate the average income every time, we would be correct on average. Of course, this fact alone tells us little about the extent to which we should trust an estimate that is based on only one realized sample. A key advantage of random sampling is, however, that it allows us (in most cases) to obtain estimates of the variability of the sampling distribution and thus of the precision of our estimates from a single sample.^[While only random sampling allows for the characterization of sampling uncertainty, random sampling does not guarantee the ability to obtain uncertainty estimates [see @kish1965survey p.24 for a discussion]. A design that samples only one cluster at random, for example, would not allow for the construction of standard errors and confidence intervals.] Typically, these uncertainty estimates take the form of standard errors that can be used to construct confidence intervals. 

For example, we may calculate a 95\% confidence interval for the average income. Even though we would not be certain that our particular sample estimate falls into the confidence interval, we would know that, were we to repeat the process of random sampling and confidence interval creation many times, 95\% of confidence intervals would contain the true population parameter. 

2. What distinguishes random sampling and random assignment?
==

The core feature of both random sampling and random assignment is that units are selected from some larger pool with known probabilities between zero and one. Yet, there are several key differences. In random sampling, units or elements are selected at random from a population in order to infer characteristics of the population from the sample. Random assignment, on the other hand, is used in experiments to allocate  units from a pool of experimental subjects to experimental conditions - in the easiest case to a treatment and a control group. The goal of random assignment is to draw inferences about causal effects, typically the average causal effect of the treatment on some outcome (see ["10 Strategies for Figuring out if X Caused Y"](https://egap.org/resource/10-strategies-for-figuring-out-if-x-caused-y/) for details).^[On a deeper level, however, random assignment has much in common of with random sampling. The process of randomly assigning subjects to a treatment group can be thought of as a process of drawing a random sample of treated potential outcomes in order to estimate the average treated potential outcome in the entire experimental subject pool [@splawa1990application]. ]
 

The subject pool of an experiment may or may not be a random sample from a larger population. In many cases, experiments rely on convenience samples. Combining random sampling and random assignment, however, has important benefits. Suppose we first draw a random sample of subjects from a population and then randomly assign some of these subjects to receive a treatment. That we used random assignment to allocate subjects to experimental conditions enables us to obtain an unbiased estimate of the average treatment effect among the subjects in our sample. That we randomly sampled our subjects from the population means that we can use our estimate of the average treatment effect in the sample to draw inferences about the average treatment effect in the population. In other words, experiments that are based on random samples offer straightforward ways to learn not only about sample average treatment effects (SATEs) but also about population average treatment effects (PATEs). 

Another advantage of randomly sampling experimental subjects is that subjects who have not been sampled can serve as additional control units. To illustrate, consider again the population of six units from the stylized example above. Suppose we would like to conduct an experiment, but  we only have research funds to measure outcomes for four units. Hence, we randomly sample, say, units 1, 2, 3 and 5, and then randomly assign units 2 and 5 to treatment. In principle, the control group of our experiment consists of units 1 and 3. Units 4 and 6, however, by  virtue of not being sampled, have also been chosen to not receive the treatment through a random process. Hence, they can be used as additional control group units should more funds for outcome measurement become available at a later stage. 



3. How to obtain a random sample?
==

Obtaining a random sample is rarely as straightforward as the above description makes it seem. In practice, the *survey population* -- the set of elements for which data can realistically be collected -- is often more restrictive than the *target population*. For example, the target population may consist of all adults who reside in a given country, but it may be infeasible to sample certain groups like prisoners or residents of areas with a difficult security situation.

The first step in random sampling is to acquire a *sampling frame*, a set of materials that are used to identify the elements of the target  population. In its simplest form, a sampling frame may be a list of elements. Sampling frames are rarely perfect and tend to be one of the sources of differences between the target and survey population. Sampling frames may suffer from both undercoverage (elements that are part of the target population are missing from the sampling frame) and overcoverage (the sampling frame includes elements that are not part of the population). See @kish1965survey[chapter 2] and @groves2011survey[chapter 3] for more on sampling frame issues and how to remedy them.

Depending in part on the nature of the sampling frame, there are different ways to randomly sample elements. Many of these have analogues among the ways in which subjects can be randomly assigned to experimental conditions (see ["10 Things to Know About Randomization"](https://egap.org/resource/10-things-to-know-about-randomization/)): 

## Simple Random Sampling

Simple random sampling is the most basic survey design. In this design, each sample of size $n$ and hence each element has the same probability of being sampled. One way of drawing a simple random sample of size $n$ from a sampling frame with $N$ elements is to enumerate all possible samples of size $n$ and randomly select one of them. This is the procedure described in the example above. However, this approach tends to be impractical in real world application, since actual populations will be much larger than $N=6$ and the number of possible samples will thus be vast. An alternative procedure is to number all elements from 1 to $N$, to generate $n$ random numbers and to select the corresponding elements.


## Stratification 

Suppose we know upfront that the characteristic in which we are interested varies across sub-populations. For example, if we aim to estimate average income, we may suspect that men earn more than women. If we draw a simple random sample, it is possible that we end up with a sample that contains way more women than men. In this case, our average income estimate is likely to be misleading. A way to guard against this possibility is to divide the population into subgroups, also called *strata*, and draw an independent random sample in each stratum. For example, we may draw an independent random sample of women and and one of men. This procedure fixes the proportion of women and men in the sample, thereby guarding against "bad" samples and improving the pricision of our estimates. Being able to ensure that the sample contains enough elements from a particular sub-population is also helpful if estimates among the sub-population are of independent interest. If we would like to estimate the gender pay gap, for example, we need a sample that contains enough men and women to obtain sufficiently precise estimates of each group's average income. 


## Clustering

Suppose we would like to estimate the average income among residents of a city. Our sample frame may not identify individual residents but we may  have access to a list of households. Instead of directly sampling residents, we can randomly select households and interview all members of the selected households. In this case, the *primary sampling units* -- the units that can be selected -- differ from the elements on which measurements are taken. Households serve as PSUs or *clusters*, while household members serve as elements.  Cluster sampling has one core downside. To the degree that elements within clusters are similar, cluster sampling will lead to a loss in precision. For example, members of the same household may have similar incomes or views.  Nonetheless, cluster sampling may be necessary if sampling frames of individual elements cannnot be obtained.  Cluster sampling may also save survey costs. For example, suppose we would like to estimate household-level income. Simple random sampling may lead to a sample of households that is dispersed throughout the city, which increases transportation costs. Instead, we may select entire blocks and interview all households within the selected blocks. Doing so may make it possible to interview more hosueholds with a smaller budget. Whether or not the gain in precision from a bigger sample size outweights the loss in precision from clustering will depend on the degree to which households within the same block are similar in terms of income. 


## Multi-stage sampling

Instead of sampling all elements in a cluster, one may draw a sub-sample of elements. For example, instead of interviewing all members, one could sample two members in each sampled household. Additional stages can be added to this approach. For example, we could first draw a sample of blocks, then a sub-sample of households within each sampled block and finally a sub-sample of household members within each sampled household. In this example, households would be referred to as *secondary sampling units* and household members as *tertiary sampling units*. One advantage of multi-stage sampling is that it allows researchers to navigate the trade-off between precision and cost-effectiveness presented by the choice between cluster sampling and simple random sampling. Increasing the number of clusters and sub-sampling fewer elements per cluster can yield a more diverse sample and hence reduce sampling variability. Yet, doing so may also increase survey costs. 

4. What to do if there does not exist an obvious sampling frame?
==

In many cases, an obvious sampling frame in the form of a list of elements or clusters of elements does not exist, especially when it comes to survey research in developing contexts. In practice, researchers therefore often rely on some variant of multi-stage *area sampling*. For example, they may first randomly sample districts, then randomly select villages within districts, then households within villages and finally individuals within households. Lists of administrative units often exist at higher levels of aggregation, but the problem of non-existent lists and maps of appropriate sampling units kicks in for the more fine-grained stages of the process. The advantage of the multi-stage approach is that such materials only have to be created for units that have actually been sampled. Mapping all households in a country, for example, may be infeasible. Listing all households in villages that have entered the sample at the previous stage, however, may be realistic. 

Where the smallest administrative unit is still too large or otherwise inappropriate as a sampling unit, researchers have resorted to creating their own *enumeration areas* (EAs). @scacco2010riots, for example, mapped 120 original EAs in order to randomly sample residents of two Nigerian cities. Similarly, @green2020countering used satellite images to create circular EAs that map onto the catchment areas of video halls in rural Uganda. This exercise was part of a field experiment that sought to evaluate the effects of a randomized media campaign.

Irrespective of whether the next larger sampling unit was pre-existing or self-created, lists of households within those sampling units rarely exist. Two popular solutions to this problem are to use a *random walk procedure* or to conduct a *household listing*.

*Random walk procedure.* Enumerators are given (possibly randomly selected) starting points and are instructed to follow a particular walk pattern, e.g. "walk to the right and conduct an interview in every fifth dwelling." The main advantage of this approach is its cost effectiveness. No listing of households beyond those that enter the sample is required. Yet, a random walk procedure may not yield a particularly good approximation to a random sample. First, even though it is often treated as a way to select a simple random sample, a random walk procedure in fact introduces geographic clustering. The walk pattern above, for example, implies that neighboring houses can never both be sampled and that once a particular household is sampled, the one that lives five houses down the road will be sampled as well. See @lohr2009sampling[chapter 5.5] for more on the consequences of this kind of clustering. Second, even if starting points are, say, randomly chosen geo-coordinates, the probability of being sampled may vary across hosueholds due to geographic pacticularities, such as the street layout, density of dwellings etc. If selection  probabilities are not known to the researcher but correlated with the population characteristic of interest, bias will result. Finally, a random walk procedure is difficult to implement in practice. One problem is that enumerators typically select households and conduct interviews in one step. This procedure increases the incentives of enumerators to informally replace households that are reluctant or unavailable -- a practice that can only be detected through intensive monitoring. 

*Household listing.* The more principled option is to create a list of all households in the sampling unit. Households can then be randomly selected from this list prior to the start of the survey. A listing exercise can be expensive, but is also more likely to allow the researcher to approximate a  random sample. Sampling from a full list of households avoids problems of geographic clustering and unknown sampling probabilities. Moreover, separating the listing from the enumeration step makes it easier to guard against unwarrented replacements. At the listing stage, enumerators have fewer incentives to skip houses, since it is not yet clear whether a household will cooeprate at the survey stage. That a record of all households in- and outside the sample exists prior to the start of survey activities can also make it easier to  detect instances in which enumerators interviewed the incorrect household. Finally, it is sometimes possible to re-use household lists to draw additional random samples in the future, at least in places where mobility is low. This feature opens up the possibility of sharing the costs of a listing exercise across multiple studies.
        
5. What to consider when putting a sampling strategy into practice?
==

The practical implementation of any sampling strategy typically presents numerous obstacles. Many unforeseen problems can occur and these are often context-specific. Hence, pre-testing one's sampling strategy is at least as if not more important than pre-testing one's questionnaire. Here are some examples of difficulties that one may be able to discover and address during a pre-test:

  - The definition of a "dwelling" or "household" can vary across contexts. It is important to clarify the definition that should be used during the household listing stage. Say, for example, that parents and adult children live on the same property but in separate houses. Should enumerators list one or two households?
  - Questions also tend to arise about the kinds of households that form part of the target (or survey) population. For instance, should rooms that are rented out to students who live there only during term time be included in the household listing?
  - The ability to read maps among survey staff varies across contexts. Working with maps or satellite imagery may be impractical or require the identification of landmarks that survey staff can use to orient themselves. 
  - When sampling household members, these are often identified by gender and age (first invented by @kish1965survey). In some contexts, households members may not know their age or birthday, in which case some other marker should be used.

In general, try to keep your sampling strategy simple. Sophisticated sampling strategies may offer gains in precision, but these gains will not be realized if a sampling strategy cannot be implemented as intended. Clear and simple instructions to survey staff are of crucial importance. It often helps to seperate tasks and assign them to specialized teams.  Sampling-related tasks such as listing households, drawing the sample of households, sampling household members and making appointments can be complex. Having seperate teams for sampling and interviewing avoids overburdening enumerators and may allow for more in-depth training. See ["10 Things to Know About Survey Implementation"](https://egap.org/resource/10-things-to-know-about-survey-implementation/) for more on how to manage field work.

6. How does one's sampling strategy affect one's analysis strategy?
==
Experimentalists are accustomed to the notion that one's random assignment strategy affects one's analysis strategy. Similar logics apply when it comes to sampling. Here are two common examples:

*1. Weight by the inverse of an element's probability of being sampled if sampling probabilities vary across elements.* 

Let us denote the probability that element $i$ is included in the sample by $\pi_i$.  If we draw a simple random sample of size $n$ from a population of size $N$, every element is included in the sample with the same probability of $\pi_i = \frac{n}{N}$. Where sampling probabilities do not vary across elements, the construction of unbiased estimators of popoulation characteristics does not usually require the analyst to make use of sampling weights. In the stylized example of simple random sampling above, for instance, we used the unweighted sample average as an estimator for the average income in the population. Samples in which all elements have the same probability of being sampled are thus also referred to as *self-weighting* [ @lohr2009sampling, p. 40].

Things change if different elements are sampled with different probabilities. For example, sampling probabilities may vary across strata. Say we stratify by gender and draw a sample of 100 men and 100 women, but there are 1000 men and 1500 women in the population. In this case, each woman has a probability of $\pi_i = \frac{1}{15}$ of being sampled, while the sampling probability among men is $\pi_i = \frac{1}{10}$. If gender is correlated with the characteristic of interest (as is likely the case for income), the unweighted sample average will not give an unbiased estimate of the the population average. The solution is to weight every element by the inverse of its sampling probability:

$$\omega_i = \frac{1}{\pi_i}.$$
Men would thus receive *sampling weights* of $\omega_m = \frac{1}{\pi_i} = 10$, while women receive sampling weights of  $\omega_w = \frac{1}{\pi_i} = 15$. Sampling weights can also be interpreted as the number of population elements represented by each unit. In this example, each man in the sample represents himself and 9 other non-sampled men, while each woman represents herself and 14 other non-sampled women.

Stratification is not the only possible source of variation in sampling probabilities across elements. For example, such variation commonly results from multi-stage sampling, as in the case where one household member is sampled in every household but households are of varying size.

Finally, the same logic applies if the population parameter that one seeks to estimate is the average causal effect of a treatment. Experimentalists commonly use inverse-probability weights to obtain unbiased estimates of the SATE if the probability of being assigned to treatment varies across units (see ["10 Things to Know About Randomization"](https://egap.org/resource/10-things-to-know-about-randomization/)). If the experimental subject pool is a random sample from a larger population and the target of inference is the PATE, researchers may *also* want to weight by the inverse of a unit's probability of being sampled. See @miratrix2018worth for more on the usage of sampling weights in the estimation of PATEs.

*2. Adjust your estimates of sampling variability for clustered sampling.* 

A common mistake is to analyze a clustered sample as if it was a simple random sample. Clustering increases sampling variability if elements within clusters are similar in terms of the characteristic of interest. Researchers need to make sure that their uncertainty estimates reflect this additional variability. Suppose again that we seek to estimate average household income in a city. We randomly sample city blocks of 10 households each. If all city blocks have the same probability of being sampled,  the unweighted average of households incomes in our sample will remain an unbiased estimator for the city-wide average household income. To estimate the variance of this estimator, however, we need to consider how incomes vary between city blocks rather than between households. To the extent that households within the same block have similar incomes, the resulting estmates will reflect that the variability across blocks is large. See  @lohr2009sampling[chapter 5] for details  and for additional ways in which one may have to adjust one's analysis strategy if clusters are of unequal size.

Again, the logic extends to cases in which the target of inference is a PATE. In randomized experiments, researchers often need to adjust their uncertainty estimates to reflect the additional variability induced by cluster random assignment (see ["10 Things to Know About Cluster Randomization"](https://egap.org/resource/10-things-to-know-about-cluster-randomization/)). If the experimental subject pool is a random sample from a larger population, however, one may want to adjust for clustering at a level higher than that of treatment assignment in cases where sampling was clustered at higher levels [@abadie2017should].

7. How to take into account  variability from both random sampling and random assignment?
==

- A more general point is that if the units in an experiment are a random sample of a larger population, then our realized estimate may deviate from the true PATE for two reasons:
  - 1)  we have drawn one of many possible samples instead of performing our analysis on the entire population,
  - 2) we cannot observe all potential outcomes and have therefore chosen one of many possible ways to assign units to treatment and control. 
  - if we want to make inferences to population quantities, our uncertainty estimates should reflect both kinds of variability
- Randomization inference only considers one kind of variability 
- Combining randomization inference with bootstrapping? [IS THAT A THING?]
-  Aronow/Green/Lee: SHARP BOUNDS ON THE VARIANCE IN RANDOMIZED EXPERIMENTS:
  - Standard Neyman standard errors are conservative if we only presume variation from random assignment, can also be justified if we sample from an infinite superpopulation
  - this paper provides bounds for the case of sampling from a finite population

8. How can sampling help with problems of non-response?
==

Non-response -- the inability to obtain measurements for some elements in the sample -- is a common problem. Where non-response is not random, it has the potential to introduce bias, because it essentially undoes the random selection of elements from the population. @hansen1946problem were the first to propose *two-phase sampling*, also called *double sampling*, as a way to deal with non-response that requires minimal additional assumptions. The basic approach works as follows. First, draw a random sample of size $n$ from a population and conduct a relatively inexpensive first attempt to obtain measurements for all $n$ elements in the sample. This first measurement phase may consist of, say, a phone survey. Let $n_R$ be the number of elements for which measurements can be obtained during this first phase and $n_M$ the number of elements for which meausrements remain missing. Second, take a sub-sample of $x\%$ of the $n_M$  missing elements and make intensive efforts to obtain measurements on all elements of the sub-sample. For example, this second phase may consist of in-person interviews. If all responses in the sub-sample can be obtained, the following provides an unbiased estimator of the population average

$$\hat{\mu} = \frac{n_R}{n} \hat{\mu}_R + \frac{n_M}{n}  \hat{\mu}_M,$$

where $\hat{\mu}_R$ is the sample average among the original respondents and $\hat{\mu}_M$ the sample average among the sub-sample of non-respondents. See @lohr2009sampling[chapter 8.3] on how to estimate the variance of this estimator. 

The two-phase sampling approach is based on the core insight that concentrating efforts on a randomly selected sub-sample of non-respondents and obtaining responses from all of them is more helpful than attempting to obtain responses from all non-respondents which is bound to result in leftover missingness.  @hansen1946problem provide results on the optimal size of the original and follow-up sample given parameters such as the probability of non-response in the first phase and the cost of initial and follow-up data collection efforts.  @coppock2017combining show how two-phase sampling can be combined with worst case bounds to partially identify average treatment effects in randomized experiments. They generalize the approach by allowing for non-response not only in the initial but also in the follow-up sample. 

9. How to sample difficult-to-reach populations?
==

- Difficult to draw a random sample of populations that are rare
- Inferential target is often the prevalence of some characteristic among the target population or the size of the difficult-to-reach population itself
- Respondent-driven-sampling as one solution to this problem
- Procedure: Start with a non-random initial sample, hand out coupons through which respondents in initial sample can recruit more respondents
- The ideas is that conducting many waves reduces the influence of the initial non-random sample
- Still does not yield a probability sample because probabilities of selection are not known
- Various approximations exist, most common one thinks of sampling process as a random walk. Implies that  unconditional probabilities of selection are proportional to number of network ties. The latter can be elicited and  used for weighting.
- This model is known to be false in many ways (e.g. presumes sampling with replacement). Gile et al. 2018 describe a range of subsequent approaches that seek to relax the problematic assumptions. 

10. What else to consider when designing a sampling strategy for a randomized experiment?
==

In many cases, randomized experiments do not rely on random samples from a larger population. Yet, even if experimental subjects are sampled in non-random ways, there are important considerations to take into account when designing a sampling strategy.

*Symmetry.* First and foremost, researchers should ensure that the chosen sampling strategy maintains symmetry across treatment and control groups. For example, suppose an experiment evaluates the effect of community meetings. Selecting respondents from a radius around the community hall where the meeting took place sounds like a natural approach. Yet, this sampling strategy requires that community halls in which the community meeting would have taken place if the village had been assigned to treatment can be identified in control group villages. Otherwise, it is not possible to implement the same sampling strategy in treatment and control villages. Where the way in which units have been sampled differs across treatment and control groups, it remains unclear whether observed differences in outcomes across these groups can be attributed to effects of the treatment or to differences in how units were sampled.  This problem is called an exclusion restriction violation. One way to avoid exclusion restriction violations of this kind is to identify all subjects on whom measurements will be taken prior to random assignment.


*Spillovers.* Researchers often worry that effects of a treatment may spill over to proximate units, especially if the units in an experiment are connected through network ties. Considering the presence of spillovers when selecting one's experimental subject pool may make it possible to rule out or control spillovers by design. For example, when sampling villages or households, one may impose a minimum distance between units in the sample in order to limit interactions among residents from different experimental units.


*Desired covariate profiles.* There are many reasons to purposefully sample subjects with particular covariate profiles. Maybe the SATE among a specific subgroup is of particular theoretical interest. Sampling particular kinds of units may also help generalize experimental results beyond the setting in which they were obtained, even if subjects have not been randomly sampled from a population. @pearl2014external and @tipton2013improving show how treatment effect estimates may be "transported" to some other setting by calculating weighted averages of estimates within subgroups. This approach is based on theoretical assumptions about which and how differences between study and target settings relate to the treatment effect of interest. Crucial for our ability to use this strategy is that our subject pool in the study setting contains enough subjects in each relevant subgroup to estimate treatment effects within it. 


References
==
