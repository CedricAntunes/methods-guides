---
title: "6+ Thing You Need to Know About Cluster Randomization"
author: "Methods Guide Author: Jake Bowers"
output: html_document
---

Abstract
==
This guide[^1] and all of the code it contains is available for copying at https://github.com/bowers-illinois-edu/EgapMethodsGuides. We encourage you to copy and improve the guide. See https://guides.github.com/activities/forking/ for one workflow in which you copy the guide, make your own changes, and then request that we include your changes in the main guide.

This guide involves a lot of R code R Core Team (2014) that we use to show how things work and to enable you to experiment and also to adapt it for your own particular purposes.

[^1]: Originating author: Jake Bowers and Ashlea Rundlett, 22 Nov 2014. The guide is a live document and subject to updating by EGAP members at any time. Bowers and Rundlett are not responsible for subsequent edits or errors introduced to this guide.

1 What it is, what it isn’t, and why we use it.
==
Cluster randomized experiments allocate treatments across groups of individuals as opposed to single individuals. Nonetheless, such studies still typically measure outcomes at the level of the individual. A study randomly assigning villages to receive different development programs but measuring individual level outcomes and a study randomly assigning households to receive different voter mobilization messages but measuring the vote turnout of individuals are both cluster randomized experiments: villages and households are the assignment units and individuals are the outcome units. A study which samples villages from the experimental pool, and then randomly assigns some of the people in each village to a treatment is not a cluster randomized experiment: in this study, both the units of assignment and outcome are the individual. A study which randomly assigned villages to an intervention and then measures village level responses is also not a cluster randomized study: it is a village level study, the units of assignment and outcome are the same.

Cluster randomization should not be confused with block randomization. If an experimenter believes that baseline outcomes vary based on pre-treatment covariates _and/or_ that treatment effects will differ in substantively important ways between subgroups (also based on pre-treatment covariates), she may divide the experimental pool into groups that are homogeneous on those covariates (like gender or village size) and randomize within those blocks — effectively turning one large experiment into multiple mini-experiments. Cluster randomization can, however, be combined with block randomization. For example, one can collect clusters of individuals (villages, classrooms, schools, households, etc.) into blocks based on cluster-level covariates and then run a blocked cluster randomized experiment in order to learn about subgroup differences in treatment effects and, at the same time, increase the precision of statistical tests. In this guide, we ignore such blocking in order to focus on cluster-lever or group-level treatment assignment with individual-level measurement.

Researchers might choose to utlize cluster randomization for a number of reasons. For example, a researcher may be interested in village/school/household-level interventions in and of themselves, possibly because they want to know the effect of every individual in a given cluster being assigned to the same treatment category. Often, however, treatments are randomly assigned at the cluster level because it is too expensive or even impossible to randomize at the individual level (radio signals, for example, either go to a geographical area or don't—you can't vary reception on an individual level).

2 Why Cluster Randomization can cause problems
==
Cluster randomized experiments have at least two units of analysis: We commonly see a few large assignment units ($J$), each containing some outcome units ($n_j$) and thus the total sample size depends on both assignment and outcome units $N=∑^J_{j=1}n_j$.

Cluster randomized experiments raise two new questions for analysts. The first question is about weighting, or how to combine information from different experimental clusters into one quantity. If clusters are not all the same size  (i.e. $n_j≠n_k$  for $j≠k$ ), then an average treatment effect must be defined in a weighted fashion and the resulting estimation should also involve weights. What weights should one use? On what basis should one choose weights? One component of weights should account for the size of the cluster (larger clusters tell us more about the treatment effect than smaller clusters, all other things equal). Another component would add that homogeneous clusters (where all villagers behave in the same way in response to treatment) tell us less about the treatment effect than heterogeneous clusters (where each villager acts as if she were more or less independent of the others). If the study is blocked, then an analyst need to choose a block-weighting scheme and cluster-weighting scheme. Hansen and Bowers (2008) discuss optimal weights for precise testing in blocked and cluster-randomized designs. Imai, King, and Nall (2009) discuss weighting schemes for estimation in paired and cluster-randomizied designs.

The second question is about information. We commonly summarize the information content of a study using the total number of participants, $N$. For example, we tend to imagine that a study with 10 people has less information about the experimental intervention than a study with 100 people. Yet, two studies with $J=10$  villages and $n_j=100$  people per village may have different information about the treatment effect on individuals if, in one study, individuals within a village are more or less independent of each other versus more or less dependent. If, say, all of the individuals in any village acted exactly the same but different villages showed different outcomes, then we would have on the order of 10 pieces of information: all of the information about causal effects in that study would be at the village level. Alternatively, if the individuals within a village acted more or less independently of each other, then we would have on the order of 10 × 100=1000 pieces of information. For a given variable, we can formalize the idea that the highly dependent clusters provide less information than the highly independent clusters with the intracluster correlation coefficient. For a given variable, $x$, we can write the intracluster correlation coefficient like so:

$$ρ ≡ \frac{variance between clusters in x}{total variance in x} ≡ \frac{τ^2_x}{τ^2_x+σ^2_x}$$

where $σ^2_x$ is the variance within clusters and $τ^2_x$ is the variance across clusters. For example, Kish (1965) uses this description of dependence to define his idea of the “effective N” of a study (in the sample survey context, where samples may be clustered):

$$effective N = \frac{N}{1+(n_j−1)ρ} = \frac{J_n}{1+(n−1)ρ}$$

where the second term follows if all of the clusters are the same size ($n_1=…=n_J≡n$).

If 1000 observations arose from 10 clusters with 20 individuals within each cluster where 50% of the variation could be attributed to cluster-to-cluster differences (and not to differences within a cluster), Kish’s formula would suggest that we have the equivalent of about 19 pieces of independent information not 10 × 20=200 pieces.

The inflation in the standard errors for estimators of the average treatment effect depends on $ρ$ as well. In this simple case with 10 clusters all the same size of 20 and $ρ=.5$, the standard error of the estimator accounting for $ρ$ is $1+(n−1)ρ=10.5$ times larger than the standard error that a simple t-test from a linear regression would provide: if $Var(\hat{\bar{Y}}_{z_{ij=1}})=\frac{s^2}{n}$ then accounting for clustered assignment with same size clusters would give us $Var(\hat{\bar{Y}}_{z_{ij=1}})=\frac{s}{(J_n)(1−(n−1)ρ)}$.[^2]

[^2]: See the following pieces for more discussion in general of the problems that arise from clustered designs in the study of politics L. Stoker and Bowers (2002), Laura Stoker and Bowers (2002), Green and Vavreck (2007), Arceneaux and Nickerson (2009)

The fact that most clustered designs contain less information than observations can lead to invalid statistical inferences. If, for example, the true standard error is ten times the estimated standard error, then our confidence intervals and statistical tests will be wildly invalid — we will be rejecting the null of no effects much too often. Without accounting for the design, we will be mislead by reports that we have ample information to reject a null of no effects: we will claim that a result is “statistically significant” when it is not.

3 Statistical inference for the average treatment effects in cluster randomized experiments part 1: Design-Based Approaches
==
## 3.1 What is a standard error?

How would an estimate of the average treatment effect vary if we repeated the experiment on the same group of villages? The standard error of an estimate of the average treatment effect is one answer to this question. Below, we simulate a simple, individual-level experiment to develop intuition about what a standard error is.[^3]

[^3]: See http://egap.org/methods-guides/10-types-treatment-effect-you-should-know-about for a demonstration that the difference of means in the observed treatment and control groups is an unbiased estimator of the average treatment effect itself and what it means to be unbiased.

```{r}
N<-100
tau<-.25
y0<-rnorm(N) ## potential outcomes to control
y1<-y0+tau   ## potential outcomes to treatment are a simple function of y0
Zrealized<-sample(rep(c(0,1),N/2)) ## Assign treatment to half

fastmean<-function(x){
  ## A Fast mean calculator (see the mean.default function for all it does that we do not want to do)
  .Internal(mean(x))
}

fastvar<-function(x){
  ## See var for this. Right now it removes missing values
  .Call(stats:::C_cov,x,NULL,5,FALSE)
}

simEstAte<-function(Z,y1,y0){
  ## A function to re-assign treatment and recalculate the difference of means
  Znew<-sample(Z)
  Y<-Znew*y1+(1-Znew)*y0
  estate<-fastmean(Y[Znew==1])-fastmean(Y[Znew==0])
  return(estate)
}

set.seed(12345)
simpleResults<-replicate(100000,simEstAte(Z=Zrealized,y1=y1,y0=y0))
sd(simpleResults) ## The standard error of the estimate of the ATE.
```

Although this preceding standard error is intuitive (it is merely the standard deviation of the distribution arising from repeating the experiment), more statistics-savvy readers will recognize closed-form expressions for the standard error like the following (see Gerber and Green (2012) and Dunning (2012) for easy to read explanations and derivations of the design-based standard error of the simple estimator of the average treatment effect). If we write T as the set of all treated units and C as the set of all non treated units, we might write

$$\widehat{Var}(\hat{τ}) = s^2(Y_{i,i_∈T})/m+s^2(Y_{i,i∈C}/(n−m))$$

where $m$ is the number assigned to treatment and $s^2(x)=(1/n−1)∑^n_{i=1}(x_i–\bar{x})^2$. Here we compare the results of the simulation to this most common standard error as well as to the “true” version (which requires that we know the potential outcomes so as to calculate their covariance):

```{r, error=FALSE, warning=FALSE, message=FALSE}
## True SE (Dunning Chap 6, Gerber and Green Chap 3 and Freedman, Pisani and Purves A-32) including the covariance between the potential outcomes

V<-var(cbind(y0,y1))
varc<-V[1,1]
vart<-V[2,2]
covtc<-V[1,2]
n<-sum(Zrealized)
m<-N-n

varestATE<-((N-n)/(N-1))*(vart/n) + ((N-m)/(N-1))* (varc/m) + (2/(N-1)) * covtc
seEstATE<-sqrt(varestATE)

## And the finite sample *feasible* version (where we do not observe the potential outcomes) and so we do not have the covariance
Yobs<-Zrealized*y1+(1-Zrealized)*y0
varYc<-var(Yobs[Zrealized==0])
varYt<-var(Yobs[Zrealized==1])
fvarestATE<-(N/(N-1)) * ( (varYt/n) + (varYc/m) )
estSEEstATE<-sqrt(fvarestATE)

##  Here we use the HC2 standard error --- which Lin 2013 shows is the randomization justied SE for OLS.
library(sandwich)
library(lmtest)

lm1<-lm(Yobs~Zrealized)

## Other SEs
iidSE<- sqrt(diag(vcov(lm1)))[["Zrealized"]]

## Worth noting that if we had covariates in the model we would want this one (which is identical to the previous one without covariates).
NeymanSE<-sqrt(diag(vcovHC(lm1,type="HC2")))[["Zrealized"]]

c(simSE=sd(simpleResults),feasibleSE=estSEEstATE, trueSE=seEstATE, olsIIDSE=iidSE, NeymanDesignSE=NeymanSE)
```

We see that the feasible SE (also known as the conservative SE) and the true SE are the same to 3 digits here, where as the OLS versions are a bit smaller and the simulated standard error is also very close. These standard errors will diverge when covariates are introduced into the linear model. And, of course, the true version is rarely calculable since we don't have access to the true potential outcomes.

##3.2 Standard Errors reflecting cluster-assignment of treatment

To begin, we will create a function which simulates a cluster randomized experiment with fixed intracluster correlation.[^4]

[^4]: Code available on the github repository shows that the ICC will increase as an additive cluster-level treatment effect increases. See Mathieu et al. (2012a) and Mathieu et al. (2012b) for some code that inspired the code we use here.

```{r}
ClusteredData<-function(J,n,tau,rho){
  ## Inspired by Mathieu et al, 2012, Journal of Applied Psychology
  if (J %% 2 != 0 | n %% 2 !=0) {
    stop(paste("Number of clusters (J) and size of clusters (n) must be even."))
  }

  ## If we do not inflate the variation in the baseline potential outcome (y0)
  ## by a function of tau (cluster level additive effect), then as tau increases,
  ## the ICC will necessarily increase.

  #y0j<-rnorm(J,0,sd=sqrt(rho))
  y0j<-rnorm(J,0,sd=(1+tau)^2 * sqrt(rho))
  dat<-expand.grid(i=1:n,J=1:J)
  #dat$y0 <- rnorm(n*J,0,sd=sqrt(1-rho))+y0j[dat$J]
  dat$y0 <- rnorm(n*J,0,sd=(1+tau)^2 * sqrt(1-rho))+y0j[dat$J]
  dat$y1 <- with(dat,fastmean(y0)+tau+(y0-fastmean(y0))*(2/3)) ## give treated group mean shift of tau but also smaller variance
  dat$Zi <- ifelse(dat$J %in% sample(1:J,J/2) == TRUE, 1, 0)
  dat$Y  <- with(dat, Zi*y1 + (1-Zi)*y0)
  return(dat)
}
```

Simulate some data from a simple cluster-randomized design for other demonstrations.

```{r}
set.seed(12345)
pretendDat<-ClusteredData(J=100,n=10,tau=.25,rho=.1)
```

##3.3 Cluster-Level Analysis

When the assignment units are clusters and the analysis units are individuals, the standard error refers to repeated reassignments of treatment to clusters. One way to avoid the problem of changing the way that standard errors are calculated is to analyze the data at the level of the cluster — that is, to take averages or sums of the outcomes within the clusters, and then to treat the study as a single-level study (with weights if the clusters vary in size) (See Dunning (2012) for an argument in favor of this approach).

Hansen and Bowers (2008) also recommend a variant of this approach which we demonstrate here. They write a test statistic as a cluster-weighted difference of means: $d(z,y)=z^Ty/z^Tm−(1−z)^Ty/(1−z)^Tm$ where $m$ is a $J×1$ vector recording the size of each cluster, and $z$ and $y$ are also both $J×1$ vectors recording the treatment assignment and observed outcome of each cluster respectively. They then show that one can write a difference of means as a shifted sum, $d(z,y)=z^Ty∗(n/(m_0n_t(n–n_t)))–1^Ty/(m_0(n–n_t))$ where $m_0$ is the size of a cluster, $n$ is the size of the experimental pool of clusters, and $n_t$ is the number of treated clusters (using their notation to make study of that paper easier) and thus, that we can characterize the distribution of the difference of means using what we know about the distribution of the only random piece of this expression, the sum of the outcome in the treatment group — $z^Ty$. We demonstrate this approach here:

```{r}
## Make a data frame at the cluster level
clusterDat<-data.frame(Yj=tapply(pretendDat$Y,pretendDat$J,sum),
               Zj=tapply(pretendDat$Zi,pretendDat$J,unique))
row.names(clusterDat)<-attr(clusterDat$Zj,"dimnames")[[1]]
clustersize<-table(pretendDat$J)
clusterDat[names(clustersize),"mj"]<-clustersize
clusterDat$ids<-as.numeric(row.names(clusterDat))

## Three equivalent formulas following Hansen and Bowers 2008 for the difference of means given clustered treatment assignment (and equal sized clusters and no blocking).

dp1<-function(x,z,m){
  crossprod(z,x)/crossprod(z,m) - crossprod(1-z,x)/crossprod(1-z,m)
}

dp2<-function(x,z,m0){
  ## m0 is a scalar cluster size
  nt<-sum(z)
  nc<-sum(1-z)
  k0<-m0*nt
  k1<-m0*nc
  crossprod(z,x)/k0 - crossprod((1-z),x)/k1
}

dp3<-function(x,z,m0,nt=sum(z),n=length(z)){
  ## m0 is a scalar cluster size
  ones<-rep(1,length(x))
  crossprod(z,x)*(n/(m0*nt*(n-nt))) - crossprod(ones,x)/(m0*(n-nt))
}

## Given equal sized clusters and no blocking, this is just the difference of means:


c(dp1= with(clusterDat,dp1(Yj,Zj,mj)),
  dp2= with(clusterDat,dp2(Yj,Zj,unique(mj))),
  dp3= with(clusterDat,dp3(Yj,Zj,unique(mj))),
  meandiff = with(pretendDat,mean(Y[Zi==1])-mean(Y[Zi==0])) )
```

Here we use the Hansen and Bowers (2008) approach to calculate the standard error of the difference in means as an estimator of the average treatment effect.

```{r}
## Now we use the Hansen and Bowers 2008 approach for the variance of the cluster-level diff of means (recall var(a*x)=a^*var(x)
# Notice that we have the cluster-size in the denominator where, in the simple calc of the design-based variance of the total of Y in the treatment group we do not refer to cluster size
# We are still assuming equal cluster sizes.

## Because the dp3 representation has only one random term crossprod(z,x), we can approximate the design based variance of this test statistic dp() using the variance(crossprod(z,x)*constant)

n<-nrow(clusterDat) ## number of clusters
nt<-sum(clusterDat$Zj) ## fixed number of treatment assigned clusters

## This from Lohr on Variance of sample total.
##Vtot <- with(clusterDat,(n^2) * (1-(nt/n)) * ( var(Yj[Zj==1])/nt ))
##sqrt(Vtot)

## Testing our code
## library(survey)
## des<-svydesign(ids=~ids,data=clusterDat[clusterDat$Zj==1,],fpc=rep(.5,n/2))
##thetot<-svytotal(~Yj,design=des)
##thetot

m0<-unique(clusterDat$mj) ## assuming all clusters same size
Vdp<-(n/(m0*nt*(n-nt)))*(var(clusterDat$Yj)/m0)
sqrt(Vdp)

## We can evaluate the hypothesis of no effects with this Normal approximation:

## First show that our analytics are on target
set.seed(12345)
thedist<-with(clusterDat,replicate(10000,dp3(Yj,sample(Zj),m0)))
sd(thedist)

plot(density(thedist))
curve(dnorm(x,mean=0,sd=sqrt(Vdp)),col="red",add=TRUE)

obsmeandiff<-with(clusterDat,dp3(Yj,Zj,m0))[1,1]

pSim<-2*min( mean(thedist>=obsmeandiff),mean(thedist>=obsmeandiff))
##pH0<-2*min(pnorm(c(-1,1)*obsmeandiff,mean=0,sd=sqrt(Vdp)))
pH0<-2*(1-pnorm(abs(obsmeandiff)/sqrt(Vdp),mean=0))

c(SimulatedP=pSim,HansenBowersP=pH0)
```

And here we combine the preceding information into a general use unfunction for testing the null of no effects in large samples using the difference of means.

```{r}
hbtest<-function(x,z,m0,n=length(z),nt=sum(z)){
  ## Hansen and Bowers 2008 based test for diff of means with cluster level assignment
  ## assuming same size clusters. See also that article for other caveats about the Normal
  ## approximation used here.
  obsmeandiff<-dp3(x=x,z=z,m0=m0,nt=nt,n=n)[1,1]
  ## Returns a two tailed p-value for the test of the null of no effects
  Vdp<-(n/(m0*nt*(n-nt)))*(fastvar(x)/m0)
  ## tailp<-pnorm(obsmeandiff/sqrt(Vdp))
  ## 2*min(c(tailp,1-tailp))
  return( 2*(1-pnorm(abs(obsmeandiff)/sqrt(Vdp))) )
}

with(clusterDat,hbtest(x=Yj,z=Zj,m0=m0))
```

##3.4 Individual Level Analysis accounting for clustering

Alternatively, one could adjust the IID standard error for clustering. If all clusters are the same size, $n_1=…=n_J=n$, and the experiment has no blocking, the formula that summarizes this repetition is:

